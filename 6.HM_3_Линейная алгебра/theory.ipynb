{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Линейный оператор</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножение матрицы на вектор можно представить как его одновременный поворот и растяжение.\n",
    "\n",
    "А если рассмотреть умножение матрицы на все векторы пространства, то получится преобразование всего этого пространства или так называемый **линейный оператор**.\n",
    "\n",
    "<u>Собственные векторы и числа:</u>\n",
    "\n",
    "* **Собственный вектор** или **айгенвектор** — это вектор, который не меняет направление под действием оператора, а только растягивается или сжимается.\n",
    "\n",
    "* Коэффициент растяжения или сжатия λ («лямбда») называется **собственным числом** оператора А и его матрицы.\n",
    "\n",
    "* **Айгенпарой** называется пара, состоящая из собственного числа и соответствующего ему собственного вектора. \n",
    "\n",
    "* **Спектром матрицы** называется набор её собственных чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свойства спектра:\n",
    "\n",
    "1) На основе спектра матрицы можно понять определённость матрицы:\n",
    "    * Если все λ > 0 → матрица А **положительно определена**.\n",
    "    * Если все λ ≥ 0 → матрица А **НЕотрицательно определена**.\n",
    "    * Если все λ < 0 → матрица А **отрицательно определена**.\n",
    "    * Если все λ ≤ 0 → матрица А **НЕположительно определена**.\n",
    "\n",
    "2) Определитель матрицы равен произведению собственных чисел.\n",
    "\n",
    "3) Из пункта 2 следует, что нулевое собственное число означает вырожденность матрицы.\n",
    "\n",
    "4) Собственные векторы из разных айгенпар, то есть отвечающие разным собственным значениям, всегда линейно независимы.\n",
    "\n",
    "Отдельно отметим свойства спектра для симметричных матриц:\n",
    "\n",
    "1) У симметричных матриц всегда полный набор собственных чисел и векторов, то есть по одному собственному числу и собственному вектору на каждый столбец матрицы.\n",
    "\n",
    "2) Собственные векторы симметричных матриц всегда ортогональны, то есть скалярное произведение любых собственных векторов симметричной матрицы всегда равно 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Собственные векторы и числа</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления собственных чисел в библиотеке *numpy* предусмотрена специальная функция `np.linalg.eig()`. Эта функция возвращает кортеж, который состоит из собственных чисел и матрицы, составленной из собственных векторов, соответствующей собственным числам (собственные векторы записаны в столбцы матрицы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные числа: \n",
      " [4. 1.]\n",
      "Собственные векторы: \n",
      " [[ 0.89442719 -0.70710678]\n",
      " [ 0.4472136   0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Пример 1\n",
    "# создаем матрицу А\n",
    "A = np.array([\n",
    "    [3, 1],\n",
    "    [2, 2]\n",
    "]).T\n",
    "# вычисляем собственные числа и собственные векторы\n",
    "eig_values, eig_vectors = np.linalg.eig(A)\n",
    "print('Собственные числа: \\n', eig_values)\n",
    "print('Собственные векторы: \\n', eig_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут сразу возникают вопросы. Полученные собственные векторы вовсе не совпадают с тем, что мы получали в ручном счёте. Секрет в том, что в *numpy* при расчёте собственных векторов они автоматически стандартизируются (как векторы) — приводятся к длине, равной 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные числа: \n",
      " [13.59373746  5.03209301  1.37416954]\n",
      "Собственные векторы: \n",
      " [[ 0.45145779  0.83661458  0.10258363]\n",
      " [ 0.62348353  0.44632316 -0.77299039]\n",
      " [ 0.63832135  0.31760303  0.62606905]]\n"
     ]
    }
   ],
   "source": [
    "# Пример 2\n",
    "# создаем матрицу А\n",
    "A = np.array([\n",
    "    [1, -5, -6],\n",
    "    [4, 8, 7],\n",
    "    [5, 9, 11]\n",
    "]).T\n",
    "# вычисляем собственные числа и собственные векторы\n",
    "eig_values, eig_vectors = np.linalg.eig(A)\n",
    "print('Собственные числа: \\n', eig_values)\n",
    "print('Собственные векторы: \\n', eig_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные числа: \n",
      " [ 2.54687092e+01  1.53129080e+00  1.63252001e-14 -1.00000000e+00]\n",
      "Собственные векторы: \n",
      " [[-0.33176532 -0.6739195   0.5479715  -0.5312532 ]\n",
      " [-0.75622544 -0.67703635  0.72006173 -0.64930947]\n",
      " [-0.42446012 -0.00311685 -0.00452869 -0.11805627]\n",
      " [-0.37133334  0.2957103  -0.42569687  0.5312532 ]]\n"
     ]
    }
   ],
   "source": [
    "# Пример 3\n",
    "# создаем матрицу А\n",
    "A = np.array([\n",
    "    [1, -4, -5, -6],\n",
    "    [4, 12, 8, 7],\n",
    "    [5, 14, 9, 11],\n",
    "    [8, 15, 7, 4]\n",
    "]).T\n",
    "# вычисляем собственные числа и собственные векторы\n",
    "eig_values, eig_vectors = np.linalg.eig(A)\n",
    "print('Собственные числа: \\n', eig_values)\n",
    "print('Собственные векторы: \\n', eig_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одно из собственных чисел получилось равным 0. Это сигнал к тому, что строки или столбцы матрицы A являются линейно зависимыми, то есть матрица плохо обусловлена, а точнее — вырождена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные числа: \n",
      " [0.+1.j 0.-1.j]\n",
      "Собственные векторы: \n",
      " [[0.70710678+0.j         0.70710678-0.j        ]\n",
      " [0.        -0.70710678j 0.        +0.70710678j]]\n"
     ]
    }
   ],
   "source": [
    "# Пример 4\n",
    "# создаем матрицу А\n",
    "A = np.array([\n",
    "    [0, 1],\n",
    "    [-1, 0],\n",
    "]).T\n",
    "# вычисляем собственные числа и собственные векторы\n",
    "eig_values, eig_vectors = np.linalg.eig(A)\n",
    "print('Собственные числа: \\n', eig_values)\n",
    "print('Собственные векторы: \\n', eig_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из теоретической части мы знаем, что у данной матрицы собственных чисел нет. Но результат есть. Только какой-то странный: что за +1.j и -1.j? Забегая вперёд, скажем, что это **комплексные числа**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Пример 5\n",
    "# from sklearn import datasets \n",
    "# import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# # загружаем датасет\n",
    "# boston = datasets.load_boston()\n",
    "# #print(boston['DESCR'])\n",
    "# boston_data = pd.DataFrame(\n",
    "#     data=boston.data, #данные\n",
    "#     columns=boston.feature_names #наименования столбцов\n",
    "# )\n",
    "# boston_data['PRICE']=boston.target\n",
    "# boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Составим матрицу наблюдений\n",
    "# A = boston_data.drop('PRICE', axis=1)\n",
    "# y = boston_data[['PRICE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Затем составим корреляционную матрицу\n",
    "# C = A.corr()\n",
    "# fig = plt.figure(figsize=(10, 5))\n",
    "# sns.heatmap(C, annot=True, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d29b261a37239c389d33b435816b0259/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # вычисляем собственные числа и собственные векторы\n",
    "# eig_values, eig_vectors = np.linalg.eig(C)\n",
    "# print('Собственные числа: \\n', eig_values)\n",
    "# #print('Собственные векторы: \\n', eig_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/4ed6651a9952cee878f8dc2465bb03d6/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как корреляционная матрица диагональная, то её собственные числа всегда действительны и различны между собой. Это мы и наблюдаем. \n",
    "\n",
    "Проверим, что все собственные векторы корреляционной матрицы **ортогональны** друг другу. Это свойство понадобится нам в методе **главных компонент**.\n",
    "\n",
    "Чтобы это проверить, достаточно найти матрицу Грама $L^{T}L$, где L — матрица, составленная из собственных факторов. В результате мы должны будем получить единичную матрицу. \n",
    "\n",
    "На её главной диагонали должны быть расположены единицы, а внедиагональные элементы — скалярные произведения собственных векторов — должны быть равны 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # считаем матрицу Грамма L^T*L:\n",
    "# print(np.round(eig_vectors.T @ eig_vectors, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/46a64e2f3dc47bc0ccb2b61b547be73d/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили единичную матрицу, а значит, собственные вектора ортогональны друг другу. Мы доказали ещё одно свойство собственных чисел для диагональных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные числа: \n",
      " [20. -4. -7.]\n",
      "Собственные векторы: \n",
      " [[-0.29813912 -0.73141292  0.25078429]\n",
      " [-0.38491044  0.59044076 -0.95097448]\n",
      " [-0.87347411  0.34119621  0.18098281]]\n"
     ]
    }
   ],
   "source": [
    "# Задание 4.1\n",
    "\n",
    "A = np.array([\n",
    "    [1, 4, 13],\n",
    "    [3, -4, 7],\n",
    "    [5, 9, 12]\n",
    "]).T\n",
    "# вычисляем собственные числа и собственные векторы\n",
    "eig_values, eig_vectors = np.linalg.eig(A)\n",
    "print('Собственные числа: \\n', eig_values.round())\n",
    "print('Собственные векторы: \\n', eig_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные числа: \n",
      " [391.  46.  16.]\n"
     ]
    }
   ],
   "source": [
    "# Задание 4.2\n",
    "\n",
    "# создаем матрицу А\n",
    "A = np.array([\n",
    "    [1, 9, 4],\n",
    "    [9, 4, 7],\n",
    "    [4, 7, 12]\n",
    "]).T\n",
    "# вычисляем собственные числа и собственные векторы\n",
    "eig_values, eig_vectors = np.linalg.eig(A.T@A)\n",
    "print('Собственные числа: \\n', np.round(eig_values, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Комплексные числа</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i**, или *мнимая единица* — это комплексное число, квадрат которого равен −1.\n",
    "\n",
    "$i^2=-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/4b733d6a7b11d9edfb53e6d54fb130d9/asset-v1:Skillfactory+DSMED+2023+type@asset+block/MAT_2_unit_37_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e15def23feac2e33bf1b4dcf7eb4082e/asset-v1:Skillfactory+DSMED+2023+type@asset+block/MAT_2_unit_37_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Метод главных компонент</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача снижения размерности — это задача преобразования данных с целью уменьшения количества признаков, которые описывают объект.\n",
    "\n",
    "Основными целями снижения размерности являются:\n",
    "* Сокращение времени работы моделей машинного обучения.\n",
    "* Сокращение избыточной информации за счёт выделения наиболее влиятельных факторов.\n",
    "* Подготовка данных для визуализации.\n",
    "\n",
    "Cнижать размерность можно как **линейными**, так и **нелинейными** способами.\n",
    "\n",
    "В этом модуле мы приведём обзор именно линейных методов снижения размерности. Начнём мы с **метода главных компонент (Principal Compoment Analysis, PCA)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним классический датасет о домах в Бостоне (Boston Housing Dataset). \n",
    "\n",
    "Рассмотрим в качестве признаков DIS и NOX — это усреднённое расстояние до Employment Centres и уровень загрязнения воздуха. \n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d00a001119c5ca0186de9b90921b1e11/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-15.png)\n",
    "\n",
    "На тепловой карте видно, что корреляция между нашими признаками DIS и NOX достаточно велика и составляет –0.77: Это значит, что в районах, которые расположены ближе к Employment Centres, выше уровень загрязнения воздуха. \n",
    "\n",
    "Линейная регрессия, построенная для зависимости фактора DIS от фактора NOX:\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/5b2aa301f03e1fa178051fab35c5570a/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-16.png)\n",
    "\n",
    "Какой же фактор выбрать: NOX или DIS?\n",
    "\n",
    "Ни тот, ни другой! Наилучшим новым признаком окажется линейная комбинация из старых факторов, расположенная вдоль синей прямой.\n",
    "\n",
    "Такая линейная комбинация называется главной компонентой. А алгоритм поиска этой комбинации как раз и называется **методом главных компонент**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведём общую структуру алгоритма для случая двух факторов:\n",
    "\n",
    "1. Составить корреляционную матрицу факторов C. Она же — матрица Грама стандартизированных факторов:\n",
    "\n",
    "$$C=corr(A)=G(x_{1_{st}}, x_{2_{st}})$$\n",
    "\n",
    "2. Найти собственные числа (спектр матрицы) и соответствующие им собственные числа матрицы C, решив характеристическое уравнение:\n",
    "\n",
    "$$det(C-\\lambda E)=0$$\n",
    "\n",
    "3. Выбрать наибольшее собственное число из полученных $\\lambda^{*}=max(\\lambda_{1},\\lambda_{2})$ и соответствующий ему собственный вектор $\\vec{v^*}$.\n",
    "\n",
    "4. Координаты выбранного собственного вектора $\\vec{v_{1}^*}$ и $\\vec{v_{2}^*}$ будут являться коэффициентами линейной комбинации — главной компонентой:\n",
    "\n",
    "$$\\vec{x^*}=v_{1}^*\\vec{x_1}+ v_{2}^*\\vec{x_2}$$\n",
    "\n",
    "**P.S.**\n",
    "\n",
    "Дополнительно делается нормировка нового признака. Её можно выполнить в самом конце шага 4 после пересчёта, а можно нормировать собственные векторы на шаге 3 (так делает компьютер). Наличие или отсутствие нормировки не повлияет на большинство свойств главных компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим метод главных компонент для выбора оптимального признака в нашей задаче. Пусть NOX будет первым фактором, а DIS — вторым.\n",
    "\n",
    "1. Вычислим корреляционную матрицу C:\n",
    "\n",
    "$$corr(NOX, DIS)=-0.77$$\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e05ed4a6ee6ef6ee4e08fbc02e738d9f/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-17.jpg)\n",
    "\n",
    "2. Вычислим собственные числа и собственные вектора матрицы C:\n",
    "\n",
    "    Собственные числа:\n",
    "\n",
    "    ![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/107586cfd56e8599fb87517787a0c173/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-18.png)\n",
    "\n",
    "    Собственные векторы:\n",
    "\n",
    "    ![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/bbb7dd46518e784ebd5a8ba7adb5c55b/asset-v1:Skillfactory+DSMED+2023+type@asset+block/dst-math-ml-3-19.png)\n",
    "\n",
    "3. Выбираем максимальное из собственных чисел:\n",
    "\n",
    "$\\lambda^{*}=max(\\lambda_{1},\\lambda_{2})=max(1.77, 0.23)=1.77$ и соответствующий ему собственный вектор, это будет $\\vec{v^{*}}=v_{1}=(1, -1)^T$.\n",
    "\n",
    "4. Координаты собственного вектора $\\vec{v^*}$ будут коэффициентами для линейной комбинации старых признаков:\n",
    "\n",
    "$$NEWFACTOR =1 \\cdot NOX_{st} - 1 \\cdot DIS_{st}$$\n",
    "\n",
    "**Важно!** При составлении нового фактора нужно брать именно стандартизированные признаки: центрированные и нормированные к единичной длине:\n",
    "\n",
    "$$NOX_{st}=\\frac{NOX-NOX_{mean}}{\\left \\| NOX-NOX_{mean}\\right \\|}$$\n",
    "\n",
    "$$DIS_{st}=\\frac{DIS-DIS_{mean}}{\\left \\|DIS-DIS_{mean}\\right \\|}$$\n",
    "\n",
    "5. Далее необходимо будет нормировать полученный фактор:\n",
    "\n",
    "$$NEWFACTOR_{st}=\\frac{NEWFACTOR}{\\left \\|NEWFACTOR\\right \\|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$NEWFACTOR_{st}$ называется **главной компонентой**. Есть ещё одна главная компонента, соответствующая меньшему собственному числу $\\lambda_2$ и собственному вектору $v_2$. Для понижения размерности и борьбы с мультиколлинеарностью мы берём только первую главную компоненту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Аналтиз алгоритма:</u>\n",
    "\n",
    "1. Так как собственные векторы корреляционной матрицы ортогональны, то и новые признаки (главные компоненты) тоже будут ортогональны. Для нас это будет означать нескоррелированность.\n",
    "\n",
    "2. В случае плохой обусловленности выбор значимых главных компонент позволяет достичь меньшей потери точности по сравнению с регрессией на сырые данные.\n",
    "\n",
    "3. Можно искать не все собственные векторы, а только значимые — с «большими» собственными числами.\n",
    "\n",
    "4. Есть несколько подходов к тому, какие собственные числа считать «большими».\n",
    "\n",
    "Самый простой из них — **метод Кайзера**: значимыми считаются только те компоненты, у которых собственное число больше среднего значения всех собственных чисел:\n",
    "\n",
    "$$\\lambda_{mean}=\\frac{\\lambda_{1}+\\lambda_{2}+...+\\lambda_{k}}{k}$$\n",
    "\n",
    "Есть также **метод сломанной трости**. Иногда просто берут несколько максимальных собственных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Границы применимости:</u>\n",
    "\n",
    "1. В случае хорошо обусловленных данных обрезать главные компоненты часто не нужно, так как потери точности могут оказаться больше, чем мы хотели бы.\n",
    "\n",
    "2. Для плохо обусловленных данных МАЛЫХ размерностей (для малого количества факторов) PCA — «то, что доктор прописал».\n",
    "\n",
    "3. Для плохо обусловленных данных БОЛЬШИХ размерностей (для большого количества факторов) при попытке применить PCA «в лоб» возникают вычислительные сложности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.24  0.86 -0.29 -0.33]\n"
     ]
    }
   ],
   "source": [
    "# Задание 6.5-3\n",
    "\n",
    "import numpy as np\n",
    "x1 = np.array([1, 2, 1, 1]).T\n",
    "x2 = np.array([70, 130, 65, 60]).T\n",
    "\n",
    "C = np.array([\n",
    "[1, 0.9922],\n",
    "[0.9922, 1],\n",
    "])\n",
    "\n",
    "eig_values, eig_vectors = np.linalg.eig(C)\n",
    "\n",
    "x1_norm = (x1 - x1.mean()) / np.linalg.norm(x1)\n",
    "x2_norm = (x2 - x2.mean()) / np.linalg.norm(x2)\n",
    "\n",
    "x_new = x1_norm * eig_vectors[0][0] + x2_norm * eig_vectors[1][0]\n",
    "\n",
    "x_new_norm = (x_new - x_new.mean()) / np.linalg.norm(x_new)\n",
    "\n",
    "print(np.round(x_new_norm, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Сингулярное разложение (Singular Value Decomposition, SVD)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Теорема</u>\n",
    "\n",
    "Любую прямоугольную матрицу A размера (n, m) можно представить в виде произведения трёх матриц:\n",
    "\n",
    "$$A_{n\\times m}=U_{n \\times n} \\cdot D_{n \\times m} \\cdot V^{T}_{m \\times m}$$\n",
    "\n",
    "* $U$ - матрица размера (n, n). Все её столбцы ортогональны друг другу и имеют единичную длину. Такие матрицы называются **ортогональными**. \n",
    "\n",
    "* $D$ - матрица размера (n, m). На её главной диагонали стоят числа, называемые **сингулярными** числами, а вне главной диагонали стоят нули.\n",
    "\n",
    "* $V$ - матрица размера (m, m). Она тоже **ортогональная**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке *numpy* сингулярное разложение реализовано в функции `np.linalg.svd()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVDResult(U=array([[-0.66666667,  0.66666667, -0.33333333],\n",
       "       [-0.66666667, -0.33333333,  0.66666667],\n",
       "       [ 0.33333333,  0.66666667,  0.66666667]]), S=array([8.48528137, 4.24264069]), Vh=array([[-0.70710678, -0.70710678],\n",
       "       [-0.70710678,  0.70710678]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составляем матрицу А \n",
    "A = np.array([\n",
    "    [2, 5, -4],\n",
    "    [6, 3, 0],\n",
    "]).T\n",
    "# применяем сингулярное разложение\n",
    "np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Преимущетсва:</u>\n",
    "\n",
    "1) **Усечённое сингулярное разложение**\n",
    "\n",
    "Усечённое сингулярное разложение — это когда из всех $\\lambda_i$ выбираются только $d$ первых, самых больших собственных чисел, а остальные кладутся равными нулю. Таким образом, мы отбрасываем незначительную информацию, оставляя только наиболее отличные от 0 собственные числа и собственные вектора.\n",
    "\n",
    "Такой приём очень активно используется, например, в задачах понижения размерности, а также в рекомендательных системах. \n",
    "\n",
    "2) **Решение МНК через сингулярное разложение**\n",
    "\n",
    "Благодаря сингулярному разложению мы можем получить защиту от работы с вырожденными матрицами.\n",
    "\n",
    "Если в классической формуле нужно было вычислять обратную матрицу $(A^{T}A)^{-1}$, которая может быть вырожденной, то в новой формуле мы вычисляем обратную матрицу от диагональной матрицы $D^{-1}$. Диагональная матрица никогда не может быть вырожденной, у неё всегда есть обратная. То есть решение будет существовать всегда, даже при линейно зависимых строках и столбцах!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
