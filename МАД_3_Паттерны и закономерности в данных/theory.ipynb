{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Дескриптивная статистика</center>\n",
    "\n",
    "Предоставляет сводные данные, такие как среднее значение, медиана, мода и стандартное отклонение.\n",
    "\n",
    "Помогает определить общие характеристики и распределение данных.\n",
    "\n",
    "Получить описательную статистику можно с помощью библиотеки *pandas*. Для этого можно использовать уже знакомый нам метод `describe()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Корреляционный анализ</center>\n",
    "\n",
    "Определяет силу и направление связи между двумя или более переменными.\n",
    "\n",
    "Позволяет выявить возможные зависимости и причинно-следственные связи.\n",
    "\n",
    "Один из самых быстрых и самых простых способов определить наличие корреляции между двумя переменными состоит в том, чтобы рассмотреть их на графике рассеяния. Для этого можно использовать *scatterplot*.\n",
    "\n",
    "В случае наличия зависимости мы увидим подобный график.\n",
    "\n",
    "![image.png](https://lms.skillfactory.ru/asset-v1:Skillfactory+DSMED+2023+type@asset+block@DSMED_PRAC_3_3_1.png)\n",
    "\n",
    "Использование *pandas*:\n",
    "```python\n",
    "  import pandas as pd\n",
    "  df = pd.DataFrame({'x': [1, 3, 3, 4, 5, 7, 9, 12, 13, 15],\n",
    "   'y': [5, 7, 9, 7, 6, 12, 14, 18, 15, 22]})\n",
    "  df.plot.scatter (x=”x”, y=”y”)\n",
    "```\n",
    "\n",
    "Использование *matplotlib*:\n",
    "```python\n",
    "import pandas as pd import matplotlib.pyplot as plt df = pd.DataFrame({“x”: [1, 3, 3, 4, 5, 7, 9, 12, 13, 15], “y”: [5, 7, 9, 7, 6, 12, 14, 18, 15, 22]}) plt.scatter (df.x, df.y)\n",
    "```\n",
    "\n",
    "Использование *seaborn*:\n",
    "```python\n",
    "  import seaborn as sns\n",
    "  sns.scatterplot(data=flights_data, x=\"year\", y=\"passengers\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Расчет коэффициентов корреляции</center>\n",
    "\n",
    "Существует несколько коэффициентов корреляции, каждый из которых применяется в различных ситуациях в зависимости от типа данных и характера их распределения.\n",
    "\n",
    "1) **Коэффициент Пирсона** (Pearson’s r)\n",
    "\n",
    "Измеряет линейную связь между двумя числовыми переменными, которые распределены нормально или близко к нормальному распределению.\n",
    "\n",
    "2) **Коэффициент Спирмена** (Spearman’s rho)\n",
    "\n",
    "Измеряет монотонную связь между двумя переменными, которые могут быть как числовыми, так и порядковыми. Монотонная связь означает, что значения одной переменной увеличиваются или уменьшаются вместе со значениями другой переменной.\n",
    "\n",
    "3) **Коэффициент Кендалла** (Kendall’s tau)\n",
    "\n",
    "Также измеряет монотонную связь между двумя переменными, но является более устойчивым к выбросам (резким отклонениям от общей тенденции) и подходит для работы с небольшими выборками.\n",
    "\n",
    "4) **Коэффициент точечной бисериальной корреляции** (Point Biserial Correlation Coefficient)\n",
    "\n",
    "Измеряет связь между бинарной переменной (которая имеет только два возможных значения) и числовой переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модуль `stats` библиотеки `SciPy` содержит несколько встроенных функций для вычисления коэффициентов корреляции между признаками различной природы, а также проверки их на статистическую значимость.\n",
    "\n",
    "Импортируем библиотеку:\n",
    "```python\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "```\n",
    "Воспользуемся коэффициентом корреляции Пирсона и Спирмана:\n",
    "```python\n",
    "r = pearsonr(df['height'], df['weight'])\n",
    "print('Pearson correlation:', r[0], 'p-value:', r[1])\n",
    "# Pearson correlation: 0.3142873661362631 p-value: 0.0\n",
    "\n",
    "r = spearmanr(df['cholesterol'], df['weight'])\n",
    "print('Spearman correlation:', r[0], 'p-value:', r[1])\n",
    "# Spearman correlation: 0.13230579095093412 p-value: 2.1550632494593984e-263\n",
    "```\n",
    "Так как *p-value < 0.05* (типичное пороговое значение), то делаем вывод о том, что взаимосвязь (корреляция) между ростом и весом статистически значима."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляция между категориальными переменными не может быть измерена с помощью коэффициентов Пирсона, Спирмена и др. Для них используется **коэффициент корреляции Крамера** (V Крамера).\n",
    "\n",
    "Он находится в диапазоне от 0 до 1, где:\n",
    "* 0 указывает на отсутствие связи между двумя переменными.\n",
    "* 1 указывает на идеальную связь между двумя переменными.\n",
    "\n",
    "Он рассчитывается как: `V Крамера = √ (X2 /n) / min (c-1, r-1)`\n",
    "* *X2*: cтатистика хи-квадрат\n",
    "* *n*: общий размер выборки\n",
    "* *р*: количество рядов\n",
    "* *c*: количество столбцов\n",
    "\n",
    "Код показывает, как вычислить *V Крамера* для таблицы 2×2:\n",
    "```python\n",
    "import scipy. stats as stats\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[7,12], [9,8]])\n",
    "#Статистика теста хи-квадрат, размер выборки и минимум строк и столбцов\n",
    "X2 = stats.chi2_contingency(data, correction= False )[0]\n",
    "n = np.sum(data)\n",
    "minDim = min(data.shape)-1\n",
    "V = np.sqrt((X2/n) / minDim)\n",
    "print(V)\n",
    "# 0.1617\n",
    "```\n",
    "\n",
    "*V Крамера* оказывается равным 0,1617 , что указывает на довольно слабую связь между двумя переменными в таблице. Таким же образом можно вычислять *критерий Крамера* и для бОльших таблиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Регрессионный анализ</center>\n",
    "\n",
    "Строит математическую модель для прогнозирования значения одной переменной (зависимой переменной) на основе значений других переменных (независимых переменных).\n",
    "\n",
    "Мы хотели бы вывести уравнение, связывающее конкретную величину одной переменной, так называемой независимой переменной, с ожидаемым значением другой, зависимой переменной. Например, если наше линейное уравнение предсказывает вес при заданном росте, то рост является нашей независимой переменной, а вес — зависимой.\n",
    "\n",
    "## <center>Линейная регрессия<center>\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Полиномиальная регрессия</center>\n",
    "\n",
    "Это расширение линейной регрессии, оно используется для моделирования нелинейной взаимосвязи между зависимой переменной и независимыми переменными. Линейная регрессия смогла подогнать только линейную модель к имеющимся данным, но с полиномиальными функциями мы можем легко подогнать некоторые нелинейные отношения между целевыми и входными функциями.\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "```\n",
    "\n",
    "`degree` это степень полинома. Более высокая степень позволяет модели более точно соответствовать обучающим данным, но это также может привести к переобучению, особенно если степень слишком высока. Следовательно, степень следует выбирать в зависимости от сложности лежащих в основе отношений в данных.\n",
    "```python\n",
    "poly_features = poly.fit_transform(x.reshape(-1, 1))\n",
    "poly_reg_model = LinearRegression()\n",
    "poly_reg_model.fit(poly_features, y)\n",
    "print(poly_reg_model.intercept_, poly_reg_model.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Дерево решений</center>\n",
    "\n",
    "Дерево решений представляет собой древовидную структуру, подобную блок-схеме, где каждый внутренний узел обозначает проверку атрибута, каждая ветвь представляет результат теста, и каждый конечный узел содержит метку класса. Существует непараметрический метод, используемый для моделирования дерева решений для прогнозирования непрерывного результата.\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случайный лес</center>\n",
    "\n",
    "Случайный лес - это метод ансамбля, способный выполнять как задачи регрессии, так и классификации с использованием нескольких деревьев решений и техники, называемой начальной загрузкой и агрегацией, широко известной как пакетирование. Основная идея, стоящая за этим, заключается в объединении нескольких деревьев решений для определения конечного результата, а не в том, чтобы полагаться на отдельные деревья решений.\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Кластеризация</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяет данные на группы (кластеры) с похожими характеристиками. Помогает идентифицировать сегменты рынка, клиентские базы и другие категории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Метод k-средних</center>\n",
    "\n",
    "Цель алгоритма — минимизировать сумму квадратов внутрикластерных расстояний до центра кластера **WCSS** (within-cluster sum of squares).\n",
    "\n",
    "![image.png](https://www.dmitrymakarov.ru/wp-content/uploads/2021/07/wcss-3-1-2048x1114.jpg)\n",
    "\n",
    "Расположим несколько точек. Их количество будет равно количеству кластеров. Эти точки называются центроидами. Посчитаем расстояние от наших данных до каждого из центроидов. Логично отнести наблюдение к тому центроиду, который находится ближе.\n",
    "\n",
    "![image.png](https://lms.skillfactory.ru/asset-v1:Skillfactory+DSMED+2023+type@asset+block@DSMED_PRAC_3_5_1.png)\n",
    "\n",
    "В частности, T1 будет отнесена к C2.\n",
    "\n",
    "Таким образом, каждая точка будет отнесена к определенному центроиду (кластеру).\n",
    "\n",
    "Далее сместим центроиды в центр получившихся кластеров. Снова отнесем точки к каждому из центроидов. Некоторые точки могут перейти к другому центроиду.\n",
    "\n",
    "![image.png](https://lms.skillfactory.ru/asset-v1:Skillfactory+DSMED+2023+type@asset+block@DSMED_PRAC_3_5_2.png)\n",
    "\n",
    "Последние два шага будут повторятся до тех пор, пока точки не перестанут переходить из одного кластера в другой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует два способа выбрать количество кластеров:\n",
    "\n",
    "1) **Экспертный метод**\n",
    "\n",
    "Выбор количества кластера будет зависеть от знания о предметной области (domain knowledge).\n",
    "\n",
    "2) **Метод локтя**\n",
    "\n",
    "Можно использовать несколько вариантов количества кластеров, измерить сумму квадрата внутрикластерных расстояний и выбрать вариант, при котором сумма перестанет уменьшаться (elbow method).\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Создадим пустой список для записи показателя WCSS (нашей ошибки)\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "# Настроим параметры модели\n",
    "kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 42)\n",
    "# Обучим модель на наших данных с разным количеством кластеров\n",
    "kmeans.fit(X)\n",
    "# Для каждого кластера рассчитаем ошибку (атрибут inertia_) и поместим в список\n",
    "wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Выбор количества кластеров методом локтя')\n",
    "plt.xlabel('Количество кластеров')\n",
    "plt.ylabel('WCSS')\n",
    "```\n",
    "\n",
    "![image.png](https://lms.skillfactory.ru/asset-v1:Skillfactory+DSMED+2023+type@asset+block@DSMED_PRAC_3_5_3.png)\n",
    "\n",
    "Как видно на графике, когда мы перешли от трех до четырех кластеров, ошибка перестала существенно уменьшаться.\n",
    "\n",
    "Давайте создадим объект класса нашей модели, используя три кластера в качестве гипепараметра модели.\n",
    "```python\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 42)\n",
    "```\n",
    "\n",
    "* `n_clusters`: это количество кластеров, на которые мы хотим разбить наши наблюдения.\n",
    "\n",
    "* `init`: определяет, как мы выберем первоначальное расположение (инициализацию) центроидов; есть два варианта, (1) выбрать центроиды случайно `init = 'random'` или (2) выбрать их так, чтобы центроиды с самого начала располагались максимально далеко друг от друга `init = 'k-means++'`; второй вариант оптимальнее.\n",
    "\n",
    "* `n_init`: сколько раз алгоритм будет инициализирован, т.е. сколько раз будут выбраны центроиды до начала оптимизации; на выходе будет выбран тот вариант, где ошибка была минимальна.\n",
    "\n",
    "* `max_iter`: максимальное количество итераций алгоритма после первоначального выбора центроидов.\n",
    "\n",
    "* `random_state`: воспроизводимость результата; используется в качестве начального числа для генератора псевдослучайных чисел.\n",
    "\n",
    "Обучение и прогноз в данном случае можно сделать одним методом `.fit_predict()`.\n",
    "```python\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Выявление аномалий</center>\n",
    "\n",
    "Самым простым способом выявления аномалий в данных является построение *boxplot*, с помощью которого можно увидеть “выбросы”, но будьте осторожны, в медицинских данных часто вы можете увидеть сильно отличающиеся значения, которые на первый взгляд можно интерпретировать как аномалию или выброс, однако таковыми не являющимися. К сожалению, в таких случаях необходимо вручную рассматривать каждое подобное значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Анализ временных рядов</center>\n",
    "\n",
    "Основные концепции временных рядов:\n",
    "\n",
    "1) **Тенденция**\n",
    "\n",
    "Тренд представляет собой общее направление, в котором временной ряд движется в течение длительного периода. Указывает, увеличиваются ли значения, уменьшаются или остаются относительно постоянными. Например, уровень холестерина в крови пациента может постепенно увеличиваться или уменьшаться в течение долгого периода времени.\n",
    "\n",
    "2) **Сезонность**\n",
    "\n",
    "Сезонность относится к повторяющимся шаблонам или циклам, которые происходят с регулярными интервалами внутри временного ряда, часто соответствующим определенным единицам времени, таким как дни, недели, месяцы или сезоны. Например, количество заболеваний гриппом может повышаться в зимние месяцы, что указывает на сезонность этого заболевания.\n",
    "\n",
    "3) **Скользящее среднее**\n",
    "\n",
    "Метод скользящего среднего является распространенным приемом, используемым при анализе временных рядов для сглаживания краткосрочных колебаний и выделения долгосрочных тенденций или закономерностей в данных. Это включает в себя вычисление среднего значения набора последовательных точек данных, называемого “окном” или “скользящим окном”, по мере перемещения по временному ряду. Например, использование скользящего среднего для сглаживания ежедневных колебаний в уровне сахара в крови у пациента с диабетом для выявления общего тренда.\n",
    "\n",
    "4) **Шум**\n",
    "\n",
    "Шум, или случайные флуктуации, представляет собой нерегулярные и непредсказуемые компоненты временных рядов, которые не следуют заметному шаблону. Он вводит изменчивость, которая не связана с основным трендом или сезонностью. Например, непредсказуемые изменения в частоте сердечных сокращений пациента, которые не связаны с основным состоянием здоровья.\n",
    "\n",
    "5) **Дифференцирование**\n",
    "\n",
    "Дифференцирование используется для определения разницы в значениях указанного интервала. По умолчанию оно равно единице, мы можем указать разные значения для графиков. Это самый популярный метод удаления тенденций в данных. Например, дифференцирование данных о температуре, чтобы определить разницу в температуре между днем и ночью или между разными сезонами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Подбор гиперпараметров моделей</center>\n",
    "\n",
    "Важно понять в чём отличие параметров модели от гиперпараметров:\n",
    "\n",
    "* **Параметры** настраиваются в процессе обучения модели на данных.\n",
    "\n",
    "* **Гиперпараметры** — это характеристики модели, которые фиксируются до начала обучения.\n",
    "\n",
    "Необходимо подобрать оптимальные гиперпараметры для каждой модели, а затем сравнить модели с наилучшими параметрами. Для этого обычно используются два основных подхода:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Решетчатый поиск (Grid Search)</center>\n",
    "\n",
    "Перебор гиперпараметров по сетке — интуитивно понятный подход к их оптимизации.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Определение параметров и их значений для перебора\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Создание модели и настройка с использованием решетчатого поиска\n",
    "rf_model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Вывод наилучших гиперпараметров и оценки\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случайный поиск (Random Search)</center>\n",
    "\n",
    "Метод случайного поиска может рассмотреть более разнообразные значения гиперпараметров за то же количество итераций, что и перебор по сетке. Таким образом, случайный поиск с большей вероятностью найдет значения, которые сильнее всего влияют на качество модели. Следовательно, он также с большей вероятностью найдет наилучшую комбинацию значений гиперпараметров. Зачастую этот метод более эффективен по времени, так как проходит меньше итераций для нахождения хорошего значения.\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Определение диапазонов значений для случайного поиска\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Создание модели и настройка с использованием случайного поиска\n",
    "rf_model = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(rf_model, param_distributions=param_dist, n_iter=100, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Вывод наилучших гиперпараметров и оценки\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", random_search.best_score_)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
