{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Неоднородные СЛАУ</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с **алгоритма классической линейной регрессии по методу наименьших квадратов** (*OLS*, *Ordinary Least Squares*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений (СЛАУ)** и в общем случае записывается как:\n",
    "\n",
    "$$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1),$$\n",
    "\n",
    "где\n",
    "\n",
    "* $n$ — количество уравнений;\n",
    "* $m$ — количество переменных;\n",
    "* $x_i$ — неизвестные переменные системы;\n",
    "* $a_{ij}$ — коэффициенты системы;\n",
    "* $b_i$ — свободные члены системы.\n",
    "\n",
    "СЛАУ (1) называется **однородной**, если все свободные члены системы равны 0 $b_1=b_2=⋯=b_n=0$:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0$$\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} x_{1}+x_{2}=0 \\\\ x_{1}+2 x_{2}=0 \\end{array}\\right.$$\n",
    "\n",
    "СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0$$\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} x_{1}+x_{2}=1 \\\\ x_{1}+2 x_{2}=2 \\end{array}\\right.$$\n",
    "\n",
    "СЛАУ можно записать в матричном виде:\n",
    "\n",
    "$$\\begin{gathered} A \\vec{w}=\\vec{b} \\\\ \\left(\\begin{array}{cccc} a_{11} & a_{12} & \\ldots & a_{1 m} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} \\\\ \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} \\end{array}\\right)\\left(\\begin{array}{c} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_m \\end{array}\\right)=\\left(\\begin{array}{c} b_1 \\\\ b_2 \\\\ \\ldots \\\\ b_n \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "где\n",
    "\n",
    "* $A$ — матрица системы,  \n",
    "* $w$ — вектор неизвестных коэффициентов, \n",
    "* $b$ — вектор свободных коэффициентов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Расширенной матрицей системы $(A|b)$ неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов (записывается через вертикальную черту):\n",
    "\n",
    "$$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть исходная система будет следующей:\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$$\n",
    "\n",
    "Запишем её в матричном виде:\n",
    "\n",
    "$$\\begin{gathered} \\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\end{array}\\right)\\left(\\begin{array}{l} w_1 \\\\ w_2 \\end{array}\\right)=\\left(\\begin{array}{l} 1 \\\\ 2 \\end{array}\\right) \\\\ A \\vec{w}=\\vec{b} \\end{gathered}$$\n",
    "\n",
    "Тогда расширенная матрица системы будет иметь вид:\n",
    "\n",
    "$$(A \\mid b)=\\left(\\begin{array}{ll|l} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "1) **«Идеальная пара»**\n",
    "\n",
    "Это так называемые определённые системы линейных уравнений, имеющие единственные решения.\n",
    "\n",
    "2) **«В активном поиске»**\n",
    "\n",
    "Неопределённые системы, имеющие бесконечно много решений.\n",
    "\n",
    "3) **«Всё сложно»**\n",
    "\n",
    "Это самый интересный для нас случай — переопределённые системы, которые не имеют точных решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"Идеальная пара\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой случай решения неоднородной СЛАУ — когда система имеет единственное решение. Такие системы называются **совместными**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — теорема Кронекера — Капелли (также её называют критерием совместности системы).\n",
    "\n",
    "**Теорема Кронекера — Капелли**:\n",
    "\n",
    "Неоднородная система линейный алгебраических уравнений $A \\vec{w} = \\vec{b}$ является совместной тогда и только тогда, когда ранг матрицы системы **$A$ равен** рангу расширенной матрицы системы $(A|\\vec{b})$ и **равен** количеству независимых переменных $m$:\n",
    "\n",
    "$$rk(A) = rk(A|\\vec{b}) = m \\leftrightarrow \\exists ! \\vec{w} = (w_{1}, w_{2}, \\ldots w_m)^T$$\n",
    "\n",
    "Причём решение системы будет равно:\n",
    "\n",
    "$$\\vec{w} = A^{-1} \\vec{b}$$\n",
    "\n",
    "> Здесь значок $\\exists !$ переводится как «существует и причём единственное»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Важно!** **Ограничения** этого метода: его можно применять только для квадратных невырожденных матриц (тех, у которых определитель не равен 0).\n",
    "\n",
    "**Резюмируем**\n",
    "\n",
    "У нас есть квадратная система с $m$ неизвестных. Если ранг матрицы коэффициентов $A$ равен рангу расширенной матрицы $(A | b)$ и равен количеству переменных $(rk(A)=rk(\\vec{b})=m)$, то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет единственное решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ единственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"В активном поиске\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следствие №1 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|\\vec{b})$, но меньше, чем количество неизвестных $m$, то система имеет бесконечное множество решений:\n",
    "\n",
    "$$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем** \n",
    "\n",
    "Если ранги матриц $A$ и $(A|\\vec{b})$ всё ещё совпадают, но уже меньше количества неизвестных ($rk(A) = rk(A | \\vec{b}) < m$), значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\vec{b}$ линейно зависим со столбцами матрицы $A$, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"Всё сложно\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следствие №2 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|\\vec{b})$, то система несовместна, то есть не имеет точных решений:\n",
    "\n",
    "$$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее.\n",
    "\n",
    "Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "$$\\left\\{\\begin{array}{l} w_1+w_2=1 \\\\ w_1+2 w_2=2 \\text { или } \\\\ w_1+w_2=12 \\end{array}\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} w \\\\ w \\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)\\right.$$\n",
    "\n",
    "Обозначим приближённое решение как $\\hat{w}$. Приближением для вектора $b$ будет $\\hat{b} = A \\hat{w}$. Также введём некоторый вектор ошибок $e = b - \\hat{b} = b - A \\hat{w}$.\n",
    "\n",
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w}_1=(1, 1)^T$, то получим:\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_1=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right)=\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right) \\\\ e_1=b-A \\widehat{w}_1=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_1=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right)=\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right) \\\\ e_1=b-A \\widehat{w}_1=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "Теперь возьмём в качестве вектора $\\hat{w}_2 = (4, -1)^T$, получим:\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_2=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{c} 4 \\\\ -1 \\end{array}\\right)=\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right) \\\\ e_2=b-A \\widehat{w}_2=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ 0 \\\\ 9 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_2=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{c} 4 \\\\ -1 \\end{array}\\right)=\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right) \\\\ e_2=b-A \\widehat{w}_2=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ 0 \\\\ 9 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, но зато можно сравнить их длины.\n",
    "\n",
    "$$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$$\n",
    "\n",
    "$$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$$\n",
    "\n",
    "Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    "\n",
    "$$\\left\\|e \\right\\| \\rightarrow min$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван *методом наименьших квадратов (МНК)*. В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "> Cтоит отметить, что обычно *OLS*-оценку (*МНК*) выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    "\n",
    "> $$\\left\\|\\vec{e} \\right\\| \\rightarrow min$$\n",
    "\n",
    "> $$\\left\\|\\vec{e} \\right\\|^2 \\rightarrow min$$\n",
    "\n",
    "> $$\\left\\|\\vec{b} - A \\vec{w} \\right\\|^2 \\rightarrow min$$\n",
    "\n",
    "> Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем** \n",
    "\n",
    "Если ранг матрицы $A$ меньше ранга расширенной системы $(A|\\vec{b})$, то независимых уравнений больше, чем переменных $(rkA<(A|\\vec{b})<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.\n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов ($OLS-оценка - \\hat{b} = (A^{T}A)^{-1}\\cdot A^{T} b$), идеей которого является ортогональная проекция вектора $b$ на столбцы матрицы $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Линейная регрессия по МНК</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче регрессии обычно есть **целевая переменная**, которую мы хотим предсказать. Её, как правило, обозначают буквой $y$. Помимо целевой переменной, есть **признаки** (их также называют **факторами** или **регрессорами**). Пусть их будет $k$ штук:\n",
    "\n",
    "$$y - таргет$$\n",
    "\n",
    "$$x_1,x_2, … ,x_k - признаки / факторы / регрессоры$$\n",
    "\n",
    "В задаче регрессии есть $N$ (как правило, их действительно много) наблюдений. Это наша обучающая выборка или датасет, представленный в виде таблицы. В столбцах таблицы располагаются векторы признаков $\\vec{x_i}$.\n",
    "\n",
    "$$\\begin{gathered} \\vec{y} \\in \\mathbb{R}^N \\\\ \\overrightarrow{x_1}, \\overrightarrow{x_2}, \\ldots, \\overrightarrow{x_k} \\in \\mathbb{R}^N \\\\ \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\ldots \\\\ y_N \\end{array}\\right), \\quad\\left(\\begin{array}{c} x_{11} \\\\ x_{12} \\\\ \\ldots \\\\ x_{1 N} \\end{array}\\right), \\ldots,\\left(\\begin{array}{c} x_{k 1} \\\\ x_{k 2} \\\\ \\ldots \\\\ x_{k N} \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "В качестве регрессионной модели мы будем использовать **модель линейной регрессии**.\n",
    "\n",
    "$$y=w_0+w_1x_1+w_2x_2+…+w_kx_k,$$\n",
    "\n",
    "$$y=(\\vec{w}, \\vec{x})$$\n",
    "\n",
    "Здесь $\\vec{w}=(w_0,w_1,…,w_k)^T$ обозначают веса (коэффициенты уравнения линейной регрессии), а $\\vec{x}=(1,x_1, x_2,…, x_k)^T$.\n",
    "\n",
    "> Наличие коэффициента $w_0$ говорит о том, что мы строим регрессию с константой, или, как ещё иногда говорят, с **интерсептом** (вектор из единиц, он же **регрессор-константа**).\n",
    "\n",
    "Как правило, $N$ гораздо больше $k$ (количество строк с данными в таблице намного больше количества столбцов) и система переопределена, значит точного решения нет. Поэтому можно найти только приближённое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Финальная формула *OLS*-оценки для коэффициентов:\n",
    "\n",
    "$$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример на Python\n",
    "# Загрузка библиотек\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "from sklearn import datasets # для импорта данных\n",
    "import seaborn as sns # для визуализации статистических данных\n",
    "import matplotlib.pyplot as plt # для построения графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
    "# boston_data = pd.read_csv('data/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "# boston_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # составляем матрицу А и вектор целевой переменной\n",
    "# CRIM = boston_data['CRIM']\n",
    "# RM = boston_data['RM']\n",
    "# A = np.column_stack((np.ones(506), CRIM, RM))\n",
    "# y = boston_data[['PRICE']]\n",
    "# print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # вычислим OLS-оценку для коэффициентов\n",
    "# w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "# print(w_hat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Теперь составим прогноз нашей модели\n",
    "# # добавились новые данные:\n",
    "# CRIM_new = 0.1\n",
    "# RM_new = 8\n",
    "# # делаем прогноз типичной стоимости дома\n",
    "# PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "# print(PRICE_new.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # короткий способ сделать прогноз\n",
    "# new=np.array([[1,CRIM_new,RM_new]])\n",
    "# print('prediction:', (new@w_hat).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм построения модели линейной регрессии по *МНК* реализован в классе `LinearRegression`, находящемся в модуле `sklearn.linear_model`. Для вычисления коэффициентов (обучения модели) нам достаточно передать в метод `fit()` нашу матрицу с наблюдениями и вектор целевой переменной, а для построения прогноза — вызвать метод `predict()`:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "new_prediction = model.predict(new)\n",
    "print('prediction:', new_prediction)\n",
    "```\n",
    "\n",
    "> Здесь при создании объекта класса `LinearRegression` мы указали `fit_intercept=False`, так как в нашей матрице наблюдений $A$ уже присутствует столбец с единицами для умножения на свободный член $w_0$. Его повторное добавление не имеет смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Проблемы в классической МНК-модели</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и у любого метода, у классической *OLS*-регрессии есть свои **ограничения**. Если матрица $A^T A$ вырождена (сингулярна) или близка к вырожденной, то хорошего решения у классической модели не получится. Такие данные называют **плохо обусловленными**.\n",
    "\n",
    "Борьба с вырожденностью матрицы $A^T A$ часто сводится к устранению «плохих» (зависимых) признаков. Для этого анализируют корреляционную матрицу признаков или матрицу их значений. Но иногда проблема может заключаться, например, в том, что один признак измерен в тысячных долях, а другой — в тысячах единиц. Тогда коэффициенты при них могут отличаться в миллион раз, что потенциально может привести к вырожденности матрицы $A^T A$.\n",
    "\n",
    "В устранении этой проблемы может помочь знакомая нам **нормализация/стандартизация данных**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реализации линейной регрессии в *sklearn* предусмотрена **борьба с плохо определёнными (близкими к вырожденным и вырожденными) матрицами**. Для этого используется метод под названием **сингулярное разложение (SVD)**. Данный метод позволяет всегда получать корректные значения при обращении матриц. Суть метода заключается в том, что в *OLS*-формуле мы на самом деле используем не саму матрицу $A$, а её диагональное представление из сингулярного разложения, которое гарантированно является невырожденным. Вот и весь секрет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правда, открытым остаётся вопрос: можно ли доверять коэффициентам, полученным таким способом, и интерпретировать их? \n",
    "\n",
    "В дальнейшем мы увидим, что делать этого лучше не стоит: возможна такая ситуация, при которой коэффициенты при линейно зависимых факторах, которые получаются в результате применения линейной регрессии через сингулярное разложение, могут получиться слишком большими по модулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле сингулярное разложение зашито в функцию `np.linalg.lstsq()`, которая позволяет в одну строку построить модель линейной регрессии по МНК:\n",
    "\n",
    "```python\n",
    "# классическая OLS-регрессия в numpy с возможностью получения решения даже для вырожденных матриц\n",
    "np.linalg.lstsq(A, y, rcond=None)\n",
    "```\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e26bc6270fd2878dbae71e3d17b4b8f6/asset-v1:SkillFactory+MIPTDS+SEPT22+type@asset+block/MATHML_md2_3_14.png)\n",
    "\n",
    "Функция возвращает четыре значения:\n",
    "\n",
    "1) вектор рассчитанных коэффициентов линейной регрессии;\n",
    "2) сумму квадратов ошибок, *MSE* (она не считается, если ранг матрицы $A$ меньше числа неизвестных, как в нашем случае);\n",
    "3) ранг матрицы $A$;\n",
    "4) вектор из сингулярных значений, которые как раз и оберегают нас от ошибки (о них мы поговорим позже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Стандартизация векторов</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализация** — это процесс приведения признаков к единому масштабу, например от 0 до 1. Пример — *min-max*-нормализация:\n",
    "\n",
    "$$x_{scaled} =  \\frac{x - x_{min}}{x_{max} - x_{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стандартизация** — это процесс приведения признаков к единому масштабу характеристик распределения — нулевому среднему и единичному стандартному отклонению:\n",
    "\n",
    "$$x_{scaled} =  \\frac{x - x_{mean}}{x_{std}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейной алгебре под стандартизацией вектора $\\vec{x} \\in R^n$ понимается несколько другая операция, которая проходит в два этапа:\n",
    "\n",
    "1) **Центрирование вектора** — это операция приведения среднего к 0:\n",
    "\n",
    "$$\\vec{x}_{cent} = \\vec{x} - \\vec{x}_{mean}$$\n",
    "\n",
    "2) **Нормирование вектора** — это операция приведения диапазона вектора к масштабу от -1 до 1 путём деления центрированного вектора на его длину:\n",
    "\n",
    "$$\\vec{x}_{st} =  \\frac{\\vec{x}_{cent}}{ \\| \\vec{x}_{cent} \\| }$$\n",
    "\n",
    "где $\\vec{x}_{mean}$ — вектор, составленный из среднего значения вектора $\\vec{x}$, а $\\| \\vec{x}_{cent} \\|$ — длина вектора $\\vec{x}_{cent}$.\n",
    "\n",
    "В результате стандартизации вектора всегда получается новый вектор, длина которого равна 1:\n",
    "\n",
    "$$\\| \\vec{x}_{st} \\|  = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До стандартизации мы прогоняли регрессию $y$ на регрессоры $x_1, x_2, …, x_k$ и константу. Всего получалось $k+1$ коэффициентов.\n",
    "\n",
    "После стандартизации мы прогоняем регрессию стандартизованного $y$ на стандартизованные регрессоры **без константы**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математика говорит, что регрессия исходного $y$ на исходные («сырые») признаки c константой точно такая же, как регрессия стандартизированного на стандартизированные признаки без константы. В чём же разница? Математически — ни в чём.\n",
    "\n",
    "На прогноз модели линейной регрессии, построенной по МНК, и её качество стандартизация практически не влияет. Масштабы признаков будут иметь значение только в том случае, если для поиска коэффициентов вы используете численные методы, такие как градиентный спуск (SGDRegressor из sklearn).\n",
    "\n",
    "Однако с точки зрения интерпретации важности коэффициентов разница есть. Если вы занимаетесь отбором наиболее важных признаков по значению коэффициентов линейной регрессии на нестандартизированных данных, это будет не совсем корректно: один признак может изменяться от 0 до 1, а второй — от -1000 до 1000. Коэффициенты при них также будут различного масштаба. Если же вы посмотрите оценки коэффициентов регрессии после стандартизации, то они будут в едином масштабе, что даст более цельную и объективную картину.\n",
    "\n",
    "Более важный бонус заключается в том, что **после стандартизации матрица Грама признаков** как по волшебству **превращается в корреляционную матрицу**. На свойства корреляционной матрицы опираются такие алгоритмы, как метод главных компонент и сингулярное разложение, а так как «сырая» и стандартизированная регрессия математически эквивалентны, то имеет смысл исследовать стандартизированную, а результаты обобщить на «сырую»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# составляем матрицу наблюдений без дополнительного столбца из единиц\n",
    "A = boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# стандартизируем векторы в столбцах матрицы A\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "A_st.describe().round(2)\n",
    "```\n",
    "\n",
    "> Обратите внимание, что для функции `linalg.norm()` обязательно необходимо указать параметр `axis=0`, так как по умолчанию норма считается для всей матрицы, а не для каждого столбца в отдельности.\n",
    "\n",
    "Для получения стандартизированных коэффициентов нам также понадобится стандартизация целевой переменной $y$ по тому же принципу:\n",
    "\n",
    "```python\n",
    "# стандартизируем вектор целевой переменной\n",
    "y_cent = y - y.mean()\n",
    "y_st = y_cent/np.linalg.norm(y_cent)\n",
    "```\n",
    "Формула для вычисления коэффициента та же, что и раньше, только матрица $A$ теперь заменяется на $A_{st}$, а $y$ — на $y_{st}$:\n",
    "\n",
    "```python\n",
    "# вычислим OLS-оценку для стандартизированных коэффициентов\n",
    "w_hat_st=np.linalg.inv(A_st.T@A_st)@A_st.T@y_st\n",
    "print(w_hat_st.values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем важный вывод**\n",
    "\n",
    "Для того чтобы проинтерпретировать оценки коэффициентов линейной регрессии (понять, каков будет прирост целевой переменной при изменении фактора на 1 условную единицу), нам достаточно построить линейную регрессию в обычном виде без стандартизации и получить обычный вектор $\\hat{\\vec{w}}$.\n",
    "\n",
    "Однако, чтобы корректно говорить о том, какой фактор оказывает на прогноз большее влияние, необходимо рассматривать стандартизированную оценку вектора коэффициентов $\\hat{\\vec{w}}_{st}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем на матрицу Грама для стандартизированных факторов:\n",
    "\n",
    "```python\n",
    "# матрица Грама\n",
    "A_st.T @ A_st\n",
    "```\n",
    "\n",
    "На самом деле мы с вами только что вычислили **матрицу выборочных корреляций** наших исходных факторов.\n",
    "\n",
    "> Матрицу корреляций можно получить только в том случае, если производить стандартизацию признаков как векторы (делить на длину центрированного вектора $\\vec{x}_{st}$). Другие способы стандартизации/нормализации признаков не превращают матрицу Грама в матрицу корреляций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Корреляционная матрица</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Корреляционная матрица $C$** — это матрица выборочных корреляций между факторами регрессий.\n",
    "\n",
    "$$C=corr(X)$$\n",
    "\n",
    "Корреляцию можно измерять различным способами:\n",
    "\n",
    "* корреляцией Пирсона;\n",
    "* корреляцией Спирмена;\n",
    "* корреляцией Кендалла.\n",
    "\n",
    "В этом модуле мы будем говорить именно о **корреляции Пирсона**. Она измеряет тесноту линейных связей между непрерывными числовыми факторами и может принимать значения от -1 до +1. Как и любая статистическая величина, корреляция бывает **генеральной** и **выборочной**.\n",
    "\n",
    "**Генеральная (истинная) корреляция** — это теоретическая величина, которая отражает общую линейную зависимость между случайными величинами $X_i$ и $X_j$. Забегая вперёд скажем, что данная характеристика является абстрактной и вычисляется для **генеральных совокупностей** — всех возможных реализаций $X_i$ и $X_j$. В природе такой величины не существует, она есть только в теории вероятностей.\n",
    "\n",
    "**Выборочная корреляция** — это корреляция, вычисленная на ограниченной выборке. Это уже ближе к нашей теме. Выборочная корреляция отражает линейную взаимосвязь между факторами $\\vec{x}_{i}$ и $\\vec{x}_{j}$, реализации которых представлены в выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Корреляция фактора с самим собой всегда равна 1: $c_{ii}=1$, то есть $c_{11}=c_{22}=1$. Так происходит потому, что скалярное произведение вектора с самим собой всегда даёт 1 по свойствам скалярного произведения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> В *NumPy* матрица корреляций вычисляется функцией `np.corrcoef()`:\n",
    "\n",
    "```python\n",
    "x_1 = np.array([1, 2, 6])\n",
    "x_2 = np.array([3000, 1000, 2000])\n",
    "np.corrcoef(x_1, x_2)\n",
    "```\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e2daaabe9c82df18ac6fb0820b2d1e5a/asset-v1:SkillFactory+MIPTDS+SEPT22+type@asset+block/MATHML_md2_4_27.png)\n",
    "\n",
    "> В *Pandas* матрица корреляций вычисляется методом `corr()`, вызванным от имени *DataFrame*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике корреляция с точки зрения линейной алгебры означает следующее:\n",
    "\n",
    "* Если корреляция $c_{ij} =1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и сонаправлены.\n",
    "\n",
    "* Если корреляция $c_{ij} =-1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и противонаправлены.\n",
    "\n",
    "* Если корреляция $c_{ij} =0$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ ортогональны друг другу и, таким образом, являются линейно независимыми.\n",
    "\n",
    "Во всех остальных случаях между факторами  и  существует какая-то линейная взаимосвязь, причём чем ближе модуль коэффициента корреляции к 1, тем сильнее эта взаимосвязь.\n",
    "\n",
    "|Сила связи|Значение коэффициента корреляции|\n",
    "|----------|--------------------------------|\n",
    "|Отсутствие связи или очень слабая связь|0…+/- 0.3|\n",
    "|Слабая связь|+/- 0.3…+/- 0.5|\n",
    "|Средняя связь|+/- 0.5…+/- 0.7|\n",
    "|Сильная связь|+/- 0.7…+/- 0.9|\n",
    "|Очень сильная или абсолютная связь|+/- 0.9…+/-1|\n",
    "\n",
    "Таким образом, матрица корреляций — это матрица Грама, составленная для стандартизированных столбцов исходной матрицы наблюдений $A$. Она всегда (в теории) симметричная. На главной диагонали этой матрицы стоят 1, а на местах всех остальных элементов — коэффициенты корреляции между факторами $\\vec{x}_i$ и $\\vec{x}_j$.\n",
    "\n",
    "Если коэффициент корреляции больше 0, то взаимосвязь между факторами прямая (растёт один — растёт второй), в противном случае — обратная (растёт один — падает второй)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем**\n",
    "\n",
    "* Корреляция — это мера линейной зависимости между признаками.\n",
    "\n",
    "* Чем больше по модулю корреляция между каким-нибудь фактором и целевым признаком, тем лучше:\n",
    "\n",
    "$$\\left|corr(\\vec{x}_{i}, \\vec{y}) \\right| \\rightarrow 1 - хорошо$$\n",
    "\n",
    "* Чем больше по модулю корреляция между факторами, тем хуже:\n",
    "\n",
    "$$\\left|corr(\\vec{x}_{i}, \\vec{x}_{j}) \\right| \\rightarrow 1 - плохо$$\n",
    "\n",
    "* Чем больше линейно зависимых факторов, тем меньше ранг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выделить **два неприятных случая**:\n",
    "\n",
    "1) **Чистая коллинеарность**\n",
    "\n",
    "    Некоторые факторы являются линейно зависимыми между собой. Это влечёт к уменьшению ранга матрицы факторов. Корреляции между зависимыми факторами близки к +1 или -1. Матрица корреляции вырождена.\n",
    "\n",
    "2) **Мультиколлинеарность**\n",
    "\n",
    "    Формально линейной зависимости между факторами нет, и матрица факторов имеет максимальный ранг. Однако корреляции между мультиколлинеарными факторами по-прежнему близки к +1 или -1, и матрица корреляции практически вырождена, несмотря на то что имеет максимальный ранг.\n",
    "\n",
    "Таким образом, чистая коллинеарность провоцирует больше проблем, но её легче заметить. Мультиколлинеарность же может быть скрытой, и заметить её не так просто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "# Задание 4.7\n",
    "\n",
    "v = np.array([5, 1, 2])\n",
    "u = np.array([4, 2, 8])\n",
    "print('{:.2f}'.format(np.corrcoef(u, v)[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 3\n",
      "Determinant: 0.0000005\n",
      "[[1.         0.99925473 0.99983661]\n",
      " [0.99925473 1.         0.99906626]\n",
      " [0.99983661 0.99906626 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Задание 4.8\n",
    "\n",
    "x_1 = np.array([5.1, 1.8, 2.1, 10.3, 12.1, 12.6])\n",
    "x_2 = np.array([10.2, 3.7, 4.1, 20.5, 24.2, 24.1])\n",
    "x_3 = np.array([2.5, 0.9, 1.1, 5.1, 6.1, 6.3])\n",
    "C = np.corrcoef([x_1, x_2, x_3])\n",
    "print('Rank:', np.linalg.matrix_rank(C))\n",
    "print('Determinant: {:.7f}'.format(np.linalg.det(C)))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Полиномиальная регрессия</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полином (многочлен)** от $k$ переменных $x_1, \\ x_2, \\ ..., \\ x_k$ — это выражение (функция) вида:\n",
    "\n",
    "$$P\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right)=\\sum_{I} w_{i} x_{1}^{i_{1}} x_{2}{ }^{i_{2}} \\ldots x_{k}^{i_{k}},$$\n",
    "\n",
    "где\n",
    "\n",
    "* $I=(i_1, i_2, \\ ...., \\ i_k)$  — набор из $k$ целых неотрицательных чисел — степеней полинома;\n",
    "\n",
    "* $w_I$ — числа, называемые коэффициентами полинома. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда переменная всего одна, полином будет записываться как:\n",
    "\n",
    "$$P(x) = \\sum_{I} w_i x^i = w_0 + w_1 x^1 + w_2 x^2 + ... + w_k x^k$$\n",
    "\n",
    "Видно, что на самом деле полином — это линейная комбинация из различных степеней переменной $x$, взятой с какими-то коэффициентами, причём некоторые из коэффициентов могут быть нулевыми.\n",
    "\n",
    "Максимальная степень при переменной $x$ называется **степенью полинома**.\n",
    "\n",
    "Полином степени $k$ способен описать абсолютно любую зависимость. Для этого ему достаточно задать набор наблюдений — точек, через которые он должен пройти (или пройти приблизительно). Вопрос стоит только в степени этого полинома — $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/b87e9ca1eebf310f078dd18ab27b6b91/asset-v1:SkillFactory+MIPTDS+SEPT22+type@asset+block/MATHML_md2_6_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Цель обучения** модели полиномиальной регрессии степени та же, что и для линейной регрессии: найти такие коэффициенты $w_i$, при которых ошибка между построенной функцией и обучающей выборкой была бы наименьшей из возможных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы определить количество коэффициентов в регрессии, есть формула:\n",
    "\n",
    "$$c = \\frac{n!}{(n-d)!d!},$$\n",
    "\n",
    "$$n = k + d,$$\n",
    "\n",
    "где $k$ — количество факторов, $d$ — степень полинома, а $!$ — символ факториала.\n",
    "\n",
    "Мы уже знакомились с полиномиальными признаками, генерация которых реализована в классе *PolynomialFeatures* из модуля `preprocessing`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте матрицу наблюдений $A_{poly}$ со сгенерированными полиномиальными признаками.\n",
    "\n",
    "1) Для начала составим обычную матрицу наблюдений $A$, расположив векторы в столбцах. Обратите внимание, что вектор из 1 мы не будем добавлять в матрицу (за нас это сделает генератор полиномиальных признаков):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4]\n",
      " [ 3  4  5]\n",
      " [-2  5  2]\n",
      " [ 1 -2  2]\n",
      " [ 5  4  6]\n",
      " [13 11  8]\n",
      " [ 1  3 -1]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 3, -2, 1, 5, 13, 1],\n",
    "    [3, 4, 5, -2, 4, 11, 3],\n",
    "    [4, 5, 2, 2, 6, 8, -1],\n",
    "]).T\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Затем импортируем класс `PolynomialFeatures` из библиотеки `sklearn`. Создадим объект этого класса, указав при инициализации степень полинома равной 2. Также укажем, что нам нужна генерация столбца из 1 (параметр `include_bias=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Осталось только вызвать метод `fit_transform()` от имени этого объекта и передать в него нашу матрицу наблюдений $A$. Для удобства выведем результат в виде *DataFrame*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4      5      6      7     8     9\n",
       "0  1.0   1.0   3.0  4.0    1.0    3.0    4.0    9.0  12.0  16.0\n",
       "1  1.0   3.0   4.0  5.0    9.0   12.0   15.0   16.0  20.0  25.0\n",
       "2  1.0  -2.0   5.0  2.0    4.0  -10.0   -4.0   25.0  10.0   4.0\n",
       "3  1.0   1.0  -2.0  2.0    1.0   -2.0    2.0    4.0  -4.0   4.0\n",
       "4  1.0   5.0   4.0  6.0   25.0   20.0   30.0   16.0  24.0  36.0\n",
       "5  1.0  13.0  11.0  8.0  169.0  143.0  104.0  121.0  88.0  64.0\n",
       "6  1.0   1.0   3.0 -1.0    1.0    3.0   -1.0    9.0  -3.0   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_poly = poly.fit_transform(A)\n",
    "display(pd.DataFrame(A_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили нашу матрицу $A_{poly}$. Давайте посмотрим на её столбцы:\n",
    "\n",
    "* столбец 0 — единичный, он отвечает за слагаемое с нулевой степенью полинома (любое число в степени 0 даёт единицу).\n",
    "\n",
    "* столбцы 1, 2 и 3 — это наши исходные признаки (векторы $\\vec{x}_1$, $\\vec{x}_2$ и $\\vec{x}_3$).\n",
    "\n",
    "* столбцы 4, 5 и 6 — произведения первого столбца со всеми столбцами: $\\vec{x}_1 \\vec{x}_1=\\vec{x}_{1}^{2}$, $\\vec{x}_1 \\vec{x}_2$ и $\\vec{x}_1 \\vec{x}_3$ соответственно.\n",
    "\n",
    "* столбцы 7 и 8 — произведения второго столбца со столбцами 2 и 3: $\\vec{x}_2 \\vec{x}_2=\\vec{x}_{2}^{2}$ и $\\vec{x}_2 \\vec{x}_3$.\n",
    "\n",
    "* столбец 9 — произведение третьего столбца с самим собой: $\\vec{x}_3 \\vec{x}_3=\\vec{x}_{3}^{2}$.\n",
    "\n",
    "Таким образом, при генерации полиномиальных признаков объект `PolynomialFeatures` сначала создаёт исходные факторы, затем умножает каждый из них на все факторы и повторяет процедуру. При этом, если комбинация $\\vec{x}_i \\vec{x}_j$ уже была сгенерирована ранее, то комбинация $\\vec{x}_j \\vec{x}_i$ не рассматривается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не дублировать код, объявим функцию `polynomial_regression()`. Она будет принимать на вход матрицу наблюдений, вектор ответов и степень полинома, а возвращать матрицу с полиномиальными признаками, вектор предсказаний и коэффициенты регрессии, найденные по МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(X, y, k):\n",
    "    poly = PolynomialFeatures(degree=k, include_bias=True)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    w_hat = np.linalg.inv(X_poly.T@X_poly)@X_poly.T@y\n",
    "    y_pred = X_poly @ w_hat\n",
    "    return X_poly, y_pred, w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Тут рассматривается датасет про квартиры, который у меня не прогрузился.*\n",
    "\n",
    "```python\n",
    "# считаем матрицу корреляций (без столбца из единиц)\n",
    "C = pd.DataFrame(A_poly5[:, 1:]).corr()\n",
    "# считаем ранг корреляционной матрицы\n",
    "print('Ранг корреляционной матрицы:', np.linalg.matrix_rank(C))\n",
    "# считаем количество факторов (не включая столбец из единиц)\n",
    "print('Количество факторов:', A_poly5[:, 1:].shape[1])\n",
    "# Ранг корреляционной матрицы: 110\n",
    "# Количество факторов: 125\n",
    "```\n",
    "\n",
    "```python\n",
    "# считаем матрицу корреляций (без столбца из единиц)\n",
    "C = pd.DataFrame(A_poly4[:, 1:]).corr()\n",
    "# считаем ранг корреляционной матрицы\n",
    "print('Ранг корреляционной матрицы:', np.linalg.matrix_rank(C))\n",
    "# считаем количество факторов (не включая столбец из единиц)\n",
    "print('Количество факторов:', A_poly4[:, 1:].shape[1])\n",
    "## Ранг корреляционной матрицы: 69\n",
    "## Количество факторов: 69\n",
    "```\n",
    "\n",
    "Для полинома четвёртой степени ранг матрицы корреляций максимален, то есть равен количеству факторов (не включая единичный столбец). Поэтому и коэффициенты регрессии полинома четвёртой степени находятся в адекватных пределах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим, что будет, если использовать для построения полиномиальной регрессии **реализацию из библиотеки *sklearn***. Создадим функцию `polynomial_regression_sk` — она будет делать то же самое, что и прошлая функция, но средствами *sklearn*. Дополнительно будем смотреть также стандартное отклонение (разброс) по коэффициентам регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def polynomial_regression_sk(X, y, k):\n",
    "    poly = PolynomialFeatures(degree=k, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    lr = LinearRegression().fit(X_poly, y)\n",
    "    y_pred = lr.predict(X_poly)\n",
    "    return X_poly, y_pred, lr.coef_\n",
    "\n",
    "A = boston_data[['LSTAT', 'PTRATIO', 'RM', 'CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "\n",
    "for k in range(1, 6):\n",
    "    A_poly, y_pred, w_hat = polynomial_regression_sk(A, y, k)\n",
    "    print(\n",
    "        \"MAPE для полинома степени {} — {:.2f}%, СКО — {:.0f}\".format(\n",
    "            k, mean_absolute_percentage_error(y, y_pred)*100, w_hat.std()\n",
    "        )\n",
    "\n",
    "    )\n",
    "## MAPE для полинома степени 1 — 18.20%, СКО — 2\n",
    "## MAPE для полинома степени 2 — 13.41%, СКО — 5\n",
    "## MAPE для полинома степени 3 — 12.93%, СКО — 9\n",
    "## MAPE для полинома степени 4 — 10.74%, СКО — 304\n",
    "## MAPE для полинома степени 5 — 9.02%, СКО — 17055\n",
    "```\n",
    "\n",
    "Секрет в том, что в *sklearn* для построения линейной регрессии используется не сама матрица наблюдений $A$, а её сингулярное разложение, которое гарантированно является невырожденным — из него исключаются линейно зависимые факторы. Таким образом, даже несмотря на немаксимальный ранг корреляционной матрицы, построить полином пятой степени всегда получится.\n",
    "\n",
    "Однако коэффициенты полинома пятой степени обладают значительно бόльшим разбросом, чем другие модели. Разброс будет всё больше расти при увеличении степени полинома. Коэффициенты не будут отражать реальной зависимости в данных и будут построены так, чтобы компенсировать линейную зависимость факторов, то есть будут неустойчивыми.\n",
    "\n",
    "К тому же, как мы уже знаем, чем выше степень полинома, тем выше шанс переобучения: модель может быть настолько сложной, что попросту попытается пройти через все точки в обучающем наборе данных, не уловив общей закономерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  2.5 -0. ]\n"
     ]
    }
   ],
   "source": [
    "# Задание 6.4\n",
    "\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 3, -2, 9],\n",
    "    [1, 9, 4, 81]\n",
    "]).T\n",
    "y = np.array([3, 7, -5, 21])\n",
    "print(np.round(np.linalg.inv(A.T@A)@A.T@y, 1))\n",
    "## [ 0.1  2.5 -0. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Регуляризация</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель полиномиальной регрессии третьей степени. Будем использовать данные о жилье в Бостоне и возьмём следующие четыре признака: `LSTAT`, `CRIM`, `PTRATIO` и `RM`.\n",
    "\n",
    "Для оценки качества модели будем использовать кросс-валидацию и сравнивать среднее значение метрики на тренировочных и валидационных фолдах. Кросс-валидацию организуем с помощью функции `cross_validate` из модуля `model_selection`.\n",
    "\n",
    "В качестве метрики используем среднюю абсолютную процентную ошибку — *MAPE*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    " \n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    " \n",
    "# создаём модель линейной регрессии\n",
    "lr = LinearRegression()\n",
    " \n",
    "# оцениваем качество модели на кросс-валидации, метрика — MAPE\n",
    "cv_results = cross_validate(lr, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\t\n",
    " \n",
    "## MAPE на тренировочных фолдах: 12.64 %\n",
    "## MAPE на валидационных фолдах: 24.16 %\n",
    "```\n",
    "\n",
    "Показатели качества отличаются практически в два раза, что говорит о высоком разбросе модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регуляризация** — это способ уменьшения переобучения моделей машинного обучения путём намеренного увеличения смещения модели для уменьшения её разброса.\n",
    "\n",
    "Регуляризация для линейной регрессии преследует сразу несколько целей. Однако далее мы увидим, что все эти цели на самом деле взаимосвязаны:\n",
    "\n",
    "* предотвратить переобучение модели;\n",
    "* включить в функцию потерь штраф за переобучение;\n",
    "* обеспечить существование обратной матрицы $(A^T A)^{-1}$;\n",
    "* не допустить огромных коэффициентов модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод множителей Лагранжа**. Прелесть данного метода в том, что он позволяет свести условную задачу оптимизации к безусловной, то есть благодаря Лагранжу мы можем перейти от системы к одному уравнению.\n",
    "\n",
    "Метод множителей Лагранжа говорит, что записанная система с ограничением эквивалентна следующей записи:\n",
    "\n",
    "$$L(\\vec{w}, \\alpha)=\\|\\vec{y}-A \\vec{w}\\|^{2}+\\alpha\\left(\\|\\vec{w}\\|_{L_{p}}\\right)^{p} \\rightarrow \\min$$\n",
    "\n",
    "В машинном обучении множитель Лагранжа  принято называть **коэффициентом регуляризации**. Он отвечает за «силу» регуляризации. Чем он больше, тем меньшие значения может принимать слагаемое $\\ {\\left({‖\\overrightarrow{w}‖}_{L_p}\\right)}^p$, то есть тем сильнее ограничения на норму весов. В этом и была наша цель — ограничить веса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>L2-регуляризация</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2-регуляризация (Ridge)**, или **регуляризация по Тихонову** — это регуляризация, в которой порядок нормы $p=2$. \n",
    "\n",
    "В случае L2-регуляризации мы накладываем ограничение на длину вектора весов $\\vec{w}$.\n",
    "\n",
    "Преимущество этой формулы в том, что, если $\\alpha >0$, то матрица $A^T A+\\alpha E$ гарантированно является невырожденной, даже если матрица $A^T A$ таковой не является. Так получается за счёт того, что по диагонали матрицы $A^T A$ мы добавляем поправки, которые создают линейную независимость между столбцами матрицы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что за реализацию линейной регрессии в *sklearn* отвечает класс *Ridge*. Основной параметр модели, на который стоит обратить внимание — `alpha`, коэффициент регуляризации из формулы Тихонова.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу отметим, что для успешной сходимости численных методов оптимизации, которые используются для решения задачи условной оптимизации, необходима стандартизация (нормализация) исходных данных, которая не требовалась для аналитического МНК в классической линейной регрессии (`LinearRegression`).\n",
    "\n",
    "Здесь под стандартизацией мы понимаем именно приведение распределения признака к нулевому среднему и единичному стандартному отклонению (`StandardScaler`), а не стандартизацию векторов, о которой мы говорили в этом модуле.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "```python\n",
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "# создаём модель линейной регрессии c L2-регуляризацией\n",
    "ridge = Ridge(alpha=20, solver='svd')\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(ridge, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\n",
    "## MAPE на тренировочных фолдах: 12.54 %\n",
    "## MAPE на валидационных фолдах: 17.02 %\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09 -1.71  1.91  0.73]\n"
     ]
    }
   ],
   "source": [
    "# Задание 7.4\n",
    "\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [5, 9, 4, 3, 5],\n",
    "    [15, 18, 18, 19, 19],\n",
    "    [7, 6, 7, 7, 7]\n",
    "]).T\n",
    "y = np.array([24, 22, 35, 33, 36])\n",
    "E = np.eye(4)\n",
    "# коэффициент регуляризации\n",
    "alpha = 1\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "w_hat_ridge = np.linalg.inv(A.T@A+alpha*E)@A.T@y\n",
    "print(np.round(w_hat_ridge, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>L1-регуляризация</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1-регуляризацией, Lasso (Least Absolute Shrinkage and Selection Operator)**, называется регуляризация, в которой порядок нормы $p=1$.\n",
    "\n",
    "Таким образом, в случае L1-регуляризации мы ограничиваем сумму модулей весов модели. Такая величина называется нормой Манхэттена (расстоянием городских кварталов).\n",
    "\n",
    "В *sklearn* L1-регуляризация реализована в классе *Lasso*, а заданная выше оптимизационная задача решается **алгоритмом координатного спуска (Coordinate Descent)**.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "\n",
    "# создаём модель линейной регрессии c L1-регуляризацией\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\n",
    "## MAPE на тренировочных фолдах: 12.44 %\n",
    "## MAPE на валидационных фолдах: 16.44 %\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Elastic-Net</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний вид регуляризации (хотя их на самом деле больше), который мы рассмотрим, называется **Elastic-Net (эластичная сетка)**. Это комбинация L1- и L2-регуляризации.\n",
    "\n",
    "Идея Elastic-Net состоит в том, что мы вводим ограничение как на норму весов порядка $p=1$, так и на норму порядка $p=2$.\n",
    "\n",
    "В *sklearn* эластичная сетка реализована в классе *ElasticNet* из пакета с линейными моделями — `linear_model`. За коэффициент $\\alpha$ отвечает параметр `alpha`, за коэффициент $\\lambda$ — `l1_ratio`.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
