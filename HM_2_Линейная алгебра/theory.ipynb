{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Неоднородные СЛАУ</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с **алгоритма классической линейной регрессии по методу наименьших квадратов** (*OLS*, *Ordinary Least Squares*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений (СЛАУ)** и в общем случае записывается как:\n",
    "\n",
    "$$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1),$$\n",
    "\n",
    "где\n",
    "\n",
    "* $n$ — количество уравнений;\n",
    "* $m$ — количество переменных;\n",
    "* $x_i$ — неизвестные переменные системы;\n",
    "* $a_{ij}$ — коэффициенты системы;\n",
    "* $b_i$ — свободные члены системы.\n",
    "\n",
    "СЛАУ (1) называется **однородной**, если все свободные члены системы равны 0 $b_1=b_2=⋯=b_n=0$:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0$$\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} x_{1}+x_{2}=0 \\\\ x_{1}+2 x_{2}=0 \\end{array}\\right.$$\n",
    "\n",
    "СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0$$\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} x_{1}+x_{2}=1 \\\\ x_{1}+2 x_{2}=2 \\end{array}\\right.$$\n",
    "\n",
    "СЛАУ можно записать в матричном виде:\n",
    "\n",
    "$$\\begin{gathered} A \\vec{w}=\\vec{b} \\\\ \\left(\\begin{array}{cccc} a_{11} & a_{12} & \\ldots & a_{1 m} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} \\\\ \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} \\end{array}\\right)\\left(\\begin{array}{c} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_m \\end{array}\\right)=\\left(\\begin{array}{c} b_1 \\\\ b_2 \\\\ \\ldots \\\\ b_n \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "где\n",
    "\n",
    "* $A$ — матрица системы,  \n",
    "* $w$ — вектор неизвестных коэффициентов, \n",
    "* $b$ — вектор свободных коэффициентов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Расширенной матрицей системы $(A|b)$ неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов (записывается через вертикальную черту):\n",
    "\n",
    "$$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть исходная система будет следующей:\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$$\n",
    "\n",
    "Запишем её в матричном виде:\n",
    "\n",
    "$$\\begin{gathered} \\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\end{array}\\right)\\left(\\begin{array}{l} w_1 \\\\ w_2 \\end{array}\\right)=\\left(\\begin{array}{l} 1 \\\\ 2 \\end{array}\\right) \\\\ A \\vec{w}=\\vec{b} \\end{gathered}$$\n",
    "\n",
    "Тогда расширенная матрица системы будет иметь вид:\n",
    "\n",
    "$$(A \\mid b)=\\left(\\begin{array}{ll|l} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "1) **«Идеальная пара»**\n",
    "\n",
    "Это так называемые определённые системы линейных уравнений, имеющие единственные решения.\n",
    "\n",
    "2) **«В активном поиске»**\n",
    "\n",
    "Неопределённые системы, имеющие бесконечно много решений.\n",
    "\n",
    "3) **«Всё сложно»**\n",
    "\n",
    "Это самый интересный для нас случай — переопределённые системы, которые не имеют точных решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"Идеальная пара\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой случай решения неоднородной СЛАУ — когда система имеет единственное решение. Такие системы называются **совместными**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — теорема Кронекера — Капелли (также её называют критерием совместности системы).\n",
    "\n",
    "**Теорема Кронекера — Капелли**:\n",
    "\n",
    "Неоднородная система линейный алгебраических уравнений $A \\vec{w} = \\vec{b}$ является совместной тогда и только тогда, когда ранг матрицы системы **$A$ равен** рангу расширенной матрицы системы $(A|\\vec{b})$ и **равен** количеству независимых переменных $m$:\n",
    "\n",
    "$$rk(A) = rk(A|\\vec{b}) = m \\leftrightarrow \\exists ! \\vec{w} = (w_{1}, w_{2}, \\ldots w_m)^T$$\n",
    "\n",
    "Причём решение системы будет равно:\n",
    "\n",
    "$$\\vec{w} = A^{-1} \\vec{b}$$\n",
    "\n",
    "> Здесь значок $\\exists !$ переводится как «существует и причём единственное»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Важно!** **Ограничения** этого метода: его можно применять только для квадратных невырожденных матриц (тех, у которых определитель не равен 0).\n",
    "\n",
    "**Резюмируем**\n",
    "\n",
    "У нас есть квадратная система с $m$ неизвестных. Если ранг матрицы коэффициентов $A$ равен рангу расширенной матрицы $(A | b)$ и равен количеству переменных $(rk(A)=rk(\\vec{b})=m)$, то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет единственное решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ единственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"В активном поиске\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следствие №1 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|\\vec{b})$, но меньше, чем количество неизвестных $m$, то система имеет бесконечное множество решений:\n",
    "\n",
    "$$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем** \n",
    "\n",
    "Если ранги матриц $A$ и $(A|\\vec{b})$ всё ещё совпадают, но уже меньше количества неизвестных ($rk(A) = rk(A | \\vec{b}) < m$), значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\vec{b}$ линейно зависим со столбцами матрицы $A$, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"Всё сложно\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следствие №2 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|\\vec{b})$, то система несовместна, то есть не имеет точных решений:\n",
    "\n",
    "$$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее.\n",
    "\n",
    "Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "$$\\left\\{\\begin{array}{l} w_1+w_2=1 \\\\ w_1+2 w_2=2 \\text { или } \\\\ w_1+w_2=12 \\end{array}\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} w \\\\ w \\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)\\right.$$\n",
    "\n",
    "Обозначим приближённое решение как $\\hat{w}$. Приближением для вектора $b$ будет $\\hat{b} = A \\hat{w}$. Также введём некоторый вектор ошибок $e = b - \\hat{b} = b - A \\hat{w}$.\n",
    "\n",
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w}_1=(1, 1)^T$, то получим:\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_1=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right)=\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right) \\\\ e_1=b-A \\widehat{w}_1=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_1=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right)=\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right) \\\\ e_1=b-A \\widehat{w}_1=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "Теперь возьмём в качестве вектора $\\hat{w}_2 = (4, -1)^T$, получим:\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_2=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{c} 4 \\\\ -1 \\end{array}\\right)=\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right) \\\\ e_2=b-A \\widehat{w}_2=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ 0 \\\\ 9 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_2=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{c} 4 \\\\ -1 \\end{array}\\right)=\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right) \\\\ e_2=b-A \\widehat{w}_2=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ 0 \\\\ 9 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, но зато можно сравнить их длины.\n",
    "\n",
    "$$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$$\n",
    "\n",
    "$$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$$\n",
    "\n",
    "Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    "\n",
    "$$\\left\\|e \\right\\| \\rightarrow min$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван *методом наименьших квадратов (МНК)*. В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "> Cтоит отметить, что обычно *OLS*-оценку (*МНК*) выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    "\n",
    "> $$\\left\\|\\vec{e} \\right\\| \\rightarrow min$$\n",
    "\n",
    "> $$\\left\\|\\vec{e} \\right\\|^2 \\rightarrow min$$\n",
    "\n",
    "> $$\\left\\|\\vec{b} - A \\vec{w} \\right\\|^2 \\rightarrow min$$\n",
    "\n",
    "> Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем** \n",
    "\n",
    "Если ранг матрицы $A$ меньше ранга расширенной системы $(A|\\vec{b})$, то независимых уравнений больше, чем переменных $(rkA<(A|\\vec{b})<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.\n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов ($OLS-оценка - \\hat{b} = (A^{T}A)^{-1}\\cdot A^{T} b$), идеей которого является ортогональная проекция вектора $b$ на столбцы матрицы $A$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
