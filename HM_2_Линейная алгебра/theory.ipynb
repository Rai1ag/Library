{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Неоднородные СЛАУ</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с **алгоритма классической линейной регрессии по методу наименьших квадратов** (*OLS*, *Ordinary Least Squares*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений (СЛАУ)** и в общем случае записывается как:\n",
    "\n",
    "$$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1),$$\n",
    "\n",
    "где\n",
    "\n",
    "* $n$ — количество уравнений;\n",
    "* $m$ — количество переменных;\n",
    "* $x_i$ — неизвестные переменные системы;\n",
    "* $a_{ij}$ — коэффициенты системы;\n",
    "* $b_i$ — свободные члены системы.\n",
    "\n",
    "СЛАУ (1) называется **однородной**, если все свободные члены системы равны 0 $b_1=b_2=⋯=b_n=0$:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0$$\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} x_{1}+x_{2}=0 \\\\ x_{1}+2 x_{2}=0 \\end{array}\\right.$$\n",
    "\n",
    "СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0$$\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} x_{1}+x_{2}=1 \\\\ x_{1}+2 x_{2}=2 \\end{array}\\right.$$\n",
    "\n",
    "СЛАУ можно записать в матричном виде:\n",
    "\n",
    "$$\\begin{gathered} A \\vec{w}=\\vec{b} \\\\ \\left(\\begin{array}{cccc} a_{11} & a_{12} & \\ldots & a_{1 m} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} \\\\ \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} \\end{array}\\right)\\left(\\begin{array}{c} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_m \\end{array}\\right)=\\left(\\begin{array}{c} b_1 \\\\ b_2 \\\\ \\ldots \\\\ b_n \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "где\n",
    "\n",
    "* $A$ — матрица системы,  \n",
    "* $w$ — вектор неизвестных коэффициентов, \n",
    "* $b$ — вектор свободных коэффициентов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Расширенной матрицей системы $(A|b)$ неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов (записывается через вертикальную черту):\n",
    "\n",
    "$$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть исходная система будет следующей:\n",
    "\n",
    "$$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$$\n",
    "\n",
    "Запишем её в матричном виде:\n",
    "\n",
    "$$\\begin{gathered} \\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\end{array}\\right)\\left(\\begin{array}{l} w_1 \\\\ w_2 \\end{array}\\right)=\\left(\\begin{array}{l} 1 \\\\ 2 \\end{array}\\right) \\\\ A \\vec{w}=\\vec{b} \\end{gathered}$$\n",
    "\n",
    "Тогда расширенная матрица системы будет иметь вид:\n",
    "\n",
    "$$(A \\mid b)=\\left(\\begin{array}{ll|l} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "1) **«Идеальная пара»**\n",
    "\n",
    "Это так называемые определённые системы линейных уравнений, имеющие единственные решения.\n",
    "\n",
    "2) **«В активном поиске»**\n",
    "\n",
    "Неопределённые системы, имеющие бесконечно много решений.\n",
    "\n",
    "3) **«Всё сложно»**\n",
    "\n",
    "Это самый интересный для нас случай — переопределённые системы, которые не имеют точных решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"Идеальная пара\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой случай решения неоднородной СЛАУ — когда система имеет единственное решение. Такие системы называются **совместными**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — теорема Кронекера — Капелли (также её называют критерием совместности системы).\n",
    "\n",
    "**Теорема Кронекера — Капелли**:\n",
    "\n",
    "Неоднородная система линейный алгебраических уравнений $A \\vec{w} = \\vec{b}$ является совместной тогда и только тогда, когда ранг матрицы системы **$A$ равен** рангу расширенной матрицы системы $(A|\\vec{b})$ и **равен** количеству независимых переменных $m$:\n",
    "\n",
    "$$rk(A) = rk(A|\\vec{b}) = m \\leftrightarrow \\exists ! \\vec{w} = (w_{1}, w_{2}, \\ldots w_m)^T$$\n",
    "\n",
    "Причём решение системы будет равно:\n",
    "\n",
    "$$\\vec{w} = A^{-1} \\vec{b}$$\n",
    "\n",
    "> Здесь значок $\\exists !$ переводится как «существует и причём единственное»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Важно!** **Ограничения** этого метода: его можно применять только для квадратных невырожденных матриц (тех, у которых определитель не равен 0).\n",
    "\n",
    "**Резюмируем**\n",
    "\n",
    "У нас есть квадратная система с $m$ неизвестных. Если ранг матрицы коэффициентов $A$ равен рангу расширенной матрицы $(A | b)$ и равен количеству переменных $(rk(A)=rk(\\vec{b})=m)$, то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет единственное решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ единственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"В активном поиске\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следствие №1 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|\\vec{b})$, но меньше, чем количество неизвестных $m$, то система имеет бесконечное множество решений:\n",
    "\n",
    "$$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем** \n",
    "\n",
    "Если ранги матриц $A$ и $(A|\\vec{b})$ всё ещё совпадают, но уже меньше количества неизвестных ($rk(A) = rk(A | \\vec{b}) < m$), значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\vec{b}$ линейно зависим со столбцами матрицы $A$, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Случай \"Всё сложно\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следствие №2 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|\\vec{b})$, то система несовместна, то есть не имеет точных решений:\n",
    "\n",
    "$$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее.\n",
    "\n",
    "Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "$$\\left\\{\\begin{array}{l} w_1+w_2=1 \\\\ w_1+2 w_2=2 \\text { или } \\\\ w_1+w_2=12 \\end{array}\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} w \\\\ w \\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)\\right.$$\n",
    "\n",
    "Обозначим приближённое решение как $\\hat{w}$. Приближением для вектора $b$ будет $\\hat{b} = A \\hat{w}$. Также введём некоторый вектор ошибок $e = b - \\hat{b} = b - A \\hat{w}$.\n",
    "\n",
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w}_1=(1, 1)^T$, то получим:\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_1=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right)=\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right) \\\\ e_1=b-A \\widehat{w}_1=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_1=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right)=\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right) \\\\ e_1=b-A \\widehat{w}_1=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "Теперь возьмём в качестве вектора $\\hat{w}_2 = (4, -1)^T$, получим:\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_2=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{c} 4 \\\\ -1 \\end{array}\\right)=\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right) \\\\ e_2=b-A \\widehat{w}_2=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ 0 \\\\ 9 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "$$\\begin{gathered} \\hat{b}=A \\widehat{w}_2=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\cdot\\left(\\begin{array}{c} 4 \\\\ -1 \\end{array}\\right)=\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right) \\\\ e_2=b-A \\widehat{w}_2=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)-\\left(\\begin{array}{l} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ 0 \\\\ 9 \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, но зато можно сравнить их длины.\n",
    "\n",
    "$$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$$\n",
    "\n",
    "$$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$$\n",
    "\n",
    "Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    "\n",
    "$$\\left\\|e \\right\\| \\rightarrow min$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван *методом наименьших квадратов (МНК)*. В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "> Cтоит отметить, что обычно *OLS*-оценку (*МНК*) выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    "\n",
    "> $$\\left\\|\\vec{e} \\right\\| \\rightarrow min$$\n",
    "\n",
    "> $$\\left\\|\\vec{e} \\right\\|^2 \\rightarrow min$$\n",
    "\n",
    "> $$\\left\\|\\vec{b} - A \\vec{w} \\right\\|^2 \\rightarrow min$$\n",
    "\n",
    "> Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем** \n",
    "\n",
    "Если ранг матрицы $A$ меньше ранга расширенной системы $(A|\\vec{b})$, то независимых уравнений больше, чем переменных $(rkA<(A|\\vec{b})<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.\n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов ($OLS-оценка - \\hat{b} = (A^{T}A)^{-1}\\cdot A^{T} b$), идеей которого является ортогональная проекция вектора $b$ на столбцы матрицы $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Линейная регрессия по МНК</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче регрессии обычно есть **целевая переменная**, которую мы хотим предсказать. Её, как правило, обозначают буквой $y$. Помимо целевой переменной, есть **признаки** (их также называют **факторами** или **регрессорами**). Пусть их будет $k$ штук:\n",
    "\n",
    "$$y - таргет$$\n",
    "\n",
    "$$x_1,x_2, … ,x_k - признаки / факторы / регрессоры$$\n",
    "\n",
    "В задаче регрессии есть $N$ (как правило, их действительно много) наблюдений. Это наша обучающая выборка или датасет, представленный в виде таблицы. В столбцах таблицы располагаются векторы признаков $\\vec{x_i}$.\n",
    "\n",
    "$$\\begin{gathered} \\vec{y} \\in \\mathbb{R}^N \\\\ \\overrightarrow{x_1}, \\overrightarrow{x_2}, \\ldots, \\overrightarrow{x_k} \\in \\mathbb{R}^N \\\\ \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\ldots \\\\ y_N \\end{array}\\right), \\quad\\left(\\begin{array}{c} x_{11} \\\\ x_{12} \\\\ \\ldots \\\\ x_{1 N} \\end{array}\\right), \\ldots,\\left(\\begin{array}{c} x_{k 1} \\\\ x_{k 2} \\\\ \\ldots \\\\ x_{k N} \\end{array}\\right) \\end{gathered}$$\n",
    "\n",
    "В качестве регрессионной модели мы будем использовать **модель линейной регрессии**.\n",
    "\n",
    "$$y=w_0+w_1x_1+w_2x_2+…+w_kx_k,$$\n",
    "\n",
    "$$y=(\\vec{w}, \\vec{x})$$\n",
    "\n",
    "Здесь $\\vec{w}=(w_0,w_1,…,w_k)^T$ обозначают веса (коэффициенты уравнения линейной регрессии), а $\\vec{x}=(1,x_1, x_2,…, x_k)^T$.\n",
    "\n",
    "> Наличие коэффициента $w_0$ говорит о том, что мы строим регрессию с константой, или, как ещё иногда говорят, с **интерсептом** (вектор из единиц, он же **регрессор-константа**).\n",
    "\n",
    "Как правило, $N$ гораздо больше $k$ (количество строк с данными в таблице намного больше количества столбцов) и система переопределена, значит точного решения нет. Поэтому можно найти только приближённое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Финальная формула *OLS*-оценки для коэффициентов:\n",
    "\n",
    "$$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример на Python\n",
    "# Загрузка библиотек\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "from sklearn import datasets # для импорта данных\n",
    "import seaborn as sns # для визуализации статистических данных\n",
    "import matplotlib.pyplot as plt # для построения графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
    "# boston_data = pd.read_csv('data/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "# boston_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # составляем матрицу А и вектор целевой переменной\n",
    "# CRIM = boston_data['CRIM']\n",
    "# RM = boston_data['RM']\n",
    "# A = np.column_stack((np.ones(506), CRIM, RM))\n",
    "# y = boston_data[['PRICE']]\n",
    "# print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # вычислим OLS-оценку для коэффициентов\n",
    "# w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "# print(w_hat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Теперь составим прогноз нашей модели\n",
    "# # добавились новые данные:\n",
    "# CRIM_new = 0.1\n",
    "# RM_new = 8\n",
    "# # делаем прогноз типичной стоимости дома\n",
    "# PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "# print(PRICE_new.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # короткий способ сделать прогноз\n",
    "# new=np.array([[1,CRIM_new,RM_new]])\n",
    "# print('prediction:', (new@w_hat).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм построения модели линейной регрессии по *МНК* реализован в классе `LinearRegression`, находящемся в модуле `sklearn.linear_model`. Для вычисления коэффициентов (обучения модели) нам достаточно передать в метод `fit()` нашу матрицу с наблюдениями и вектор целевой переменной, а для построения прогноза — вызвать метод `predict()`:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "new_prediction = model.predict(new)\n",
    "print('prediction:', new_prediction)\n",
    "```\n",
    "\n",
    "> Здесь при создании объекта класса `LinearRegression` мы указали `fit_intercept=False`, так как в нашей матрице наблюдений $A$ уже присутствует столбец с единицами для умножения на свободный член $w_0$. Его повторное добавление не имеет смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Проблемы в классической МНК-модели</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и у любого метода, у классической *OLS*-регрессии есть свои **ограничения**. Если матрица $A^T A$ вырождена (сингулярна) или близка к вырожденной, то хорошего решения у классической модели не получится. Такие данные называют **плохо обусловленными**.\n",
    "\n",
    "Борьба с вырожденностью матрицы $A^T A$ часто сводится к устранению «плохих» (зависимых) признаков. Для этого анализируют корреляционную матрицу признаков или матрицу их значений. Но иногда проблема может заключаться, например, в том, что один признак измерен в тысячных долях, а другой — в тысячах единиц. Тогда коэффициенты при них могут отличаться в миллион раз, что потенциально может привести к вырожденности матрицы $A^T A$.\n",
    "\n",
    "В устранении этой проблемы может помочь знакомая нам **нормализация/стандартизация данных**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реализации линейной регрессии в *sklearn* предусмотрена **борьба с плохо определёнными (близкими к вырожденным и вырожденными) матрицами**. Для этого используется метод под названием **сингулярное разложение (SVD)**. Данный метод позволяет всегда получать корректные значения при обращении матриц. Суть метода заключается в том, что в *OLS*-формуле мы на самом деле используем не саму матрицу $A$, а её диагональное представление из сингулярного разложения, которое гарантированно является невырожденным. Вот и весь секрет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правда, открытым остаётся вопрос: можно ли доверять коэффициентам, полученным таким способом, и интерпретировать их? \n",
    "\n",
    "В дальнейшем мы увидим, что делать этого лучше не стоит: возможна такая ситуация, при которой коэффициенты при линейно зависимых факторах, которые получаются в результате применения линейной регрессии через сингулярное разложение, могут получиться слишком большими по модулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле сингулярное разложение зашито в функцию `np.linalg.lstsq()`, которая позволяет в одну строку построить модель линейной регрессии по МНК:\n",
    "\n",
    "```python\n",
    "# классическая OLS-регрессия в numpy с возможностью получения решения даже для вырожденных матриц\n",
    "np.linalg.lstsq(A, y, rcond=None)\n",
    "```\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e26bc6270fd2878dbae71e3d17b4b8f6/asset-v1:SkillFactory+MIPTDS+SEPT22+type@asset+block/MATHML_md2_3_14.png)\n",
    "\n",
    "Функция возвращает четыре значения:\n",
    "\n",
    "1) вектор рассчитанных коэффициентов линейной регрессии;\n",
    "2) сумму квадратов ошибок, *MSE* (она не считается, если ранг матрицы $A$ меньше числа неизвестных, как в нашем случае);\n",
    "3) ранг матрицы $A$;\n",
    "4) вектор из сингулярных значений, которые как раз и оберегают нас от ошибки (о них мы поговорим позже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Стандартизация векторов</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализация** — это процесс приведения признаков к единому масштабу, например от 0 до 1. Пример — *min-max*-нормализация:\n",
    "\n",
    "$$x_{scaled} =  \\frac{x - x_{min}}{x_{max} - x_{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стандартизация** — это процесс приведения признаков к единому масштабу характеристик распределения — нулевому среднему и единичному стандартному отклонению:\n",
    "\n",
    "$$x_{scaled} =  \\frac{x - x_{mean}}{x_{std}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейной алгебре под стандартизацией вектора $\\vec{x} \\in R^n$ понимается несколько другая операция, которая проходит в два этапа:\n",
    "\n",
    "1) **Центрирование вектора** — это операция приведения среднего к 0:\n",
    "\n",
    "$$\\vec{x}_{cent} = \\vec{x} - \\vec{x}_{mean}$$\n",
    "\n",
    "2) **Нормирование вектора** — это операция приведения диапазона вектора к масштабу от -1 до 1 путём деления центрированного вектора на его длину:\n",
    "\n",
    "$$\\vec{x}_{st} =  \\frac{\\vec{x}_{cent}}{ \\| \\vec{x}_{cent} \\| }$$\n",
    "\n",
    "где $\\vec{x}_{mean}$ — вектор, составленный из среднего значения вектора $\\vec{x}$, а $\\| \\vec{x}_{cent} \\|$ — длина вектора $\\vec{x}_{cent}$.\n",
    "\n",
    "В результате стандартизации вектора всегда получается новый вектор, длина которого равна 1:\n",
    "\n",
    "$$\\| \\vec{x}_{st} \\|  = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До стандартизации мы прогоняли регрессию $y$ на регрессоры $x_1, x_2, …, x_k$ и константу. Всего получалось $k+1$ коэффициентов.\n",
    "\n",
    "После стандартизации мы прогоняем регрессию стандартизованного $y$ на стандартизованные регрессоры **без константы**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математика говорит, что регрессия исходного $y$ на исходные («сырые») признаки c константой точно такая же, как регрессия стандартизированного на стандартизированные признаки без константы. В чём же разница? Математически — ни в чём.\n",
    "\n",
    "На прогноз модели линейной регрессии, построенной по МНК, и её качество стандартизация практически не влияет. Масштабы признаков будут иметь значение только в том случае, если для поиска коэффициентов вы используете численные методы, такие как градиентный спуск (SGDRegressor из sklearn).\n",
    "\n",
    "Однако с точки зрения интерпретации важности коэффициентов разница есть. Если вы занимаетесь отбором наиболее важных признаков по значению коэффициентов линейной регрессии на нестандартизированных данных, это будет не совсем корректно: один признак может изменяться от 0 до 1, а второй — от -1000 до 1000. Коэффициенты при них также будут различного масштаба. Если же вы посмотрите оценки коэффициентов регрессии после стандартизации, то они будут в едином масштабе, что даст более цельную и объективную картину.\n",
    "\n",
    "Более важный бонус заключается в том, что **после стандартизации матрица Грама признаков** как по волшебству **превращается в корреляционную матрицу**. На свойства корреляционной матрицы опираются такие алгоритмы, как метод главных компонент и сингулярное разложение, а так как «сырая» и стандартизированная регрессия математически эквивалентны, то имеет смысл исследовать стандартизированную, а результаты обобщить на «сырую»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# составляем матрицу наблюдений без дополнительного столбца из единиц\n",
    "A = boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# стандартизируем векторы в столбцах матрицы A\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "A_st.describe().round(2)\n",
    "```\n",
    "\n",
    "> Обратите внимание, что для функции `linalg.norm()` обязательно необходимо указать параметр `axis=0`, так как по умолчанию норма считается для всей матрицы, а не для каждого столбца в отдельности.\n",
    "\n",
    "Для получения стандартизированных коэффициентов нам также понадобится стандартизация целевой переменной $y$ по тому же принципу:\n",
    "\n",
    "```python\n",
    "# стандартизируем вектор целевой переменной\n",
    "y_cent = y - y.mean()\n",
    "y_st = y_cent/np.linalg.norm(y_cent)\n",
    "```\n",
    "Формула для вычисления коэффициента та же, что и раньше, только матрица $A$ теперь заменяется на $A_{st}$, а $y$ — на $y_{st}$:\n",
    "\n",
    "```python\n",
    "# вычислим OLS-оценку для стандартизированных коэффициентов\n",
    "w_hat_st=np.linalg.inv(A_st.T@A_st)@A_st.T@y_st\n",
    "print(w_hat_st.values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем важный вывод**\n",
    "\n",
    "Для того чтобы проинтерпретировать оценки коэффициентов линейной регрессии (понять, каков будет прирост целевой переменной при изменении фактора на 1 условную единицу), нам достаточно построить линейную регрессию в обычном виде без стандартизации и получить обычный вектор $\\hat{\\vec{w}}$.\n",
    "\n",
    "Однако, чтобы корректно говорить о том, какой фактор оказывает на прогноз большее влияние, необходимо рассматривать стандартизированную оценку вектора коэффициентов $\\hat{\\vec{w}}_{st}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем на матрицу Грама для стандартизированных факторов:\n",
    "\n",
    "```python\n",
    "# матрица Грама\n",
    "A_st.T @ A_st\n",
    "```\n",
    "\n",
    "На самом деле мы с вами только что вычислили **матрицу выборочных корреляций** наших исходных факторов.\n",
    "\n",
    "> Матрицу корреляций можно получить только в том случае, если производить стандартизацию признаков как векторы (делить на длину центрированного вектора $\\vec{x}_{st}$). Другие способы стандартизации/нормализации признаков не превращают матрицу Грама в матрицу корреляций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Корреляционная матрица</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Корреляционная матрица $C$** — это матрица выборочных корреляций между факторами регрессий.\n",
    "\n",
    "$$C=corr(X)$$\n",
    "\n",
    "Корреляцию можно измерять различным способами:\n",
    "\n",
    "* корреляцией Пирсона;\n",
    "* корреляцией Спирмена;\n",
    "* корреляцией Кендалла.\n",
    "\n",
    "В этом модуле мы будем говорить именно о **корреляции Пирсона**. Она измеряет тесноту линейных связей между непрерывными числовыми факторами и может принимать значения от -1 до +1. Как и любая статистическая величина, корреляция бывает **генеральной** и **выборочной**.\n",
    "\n",
    "**Генеральная (истинная) корреляция** — это теоретическая величина, которая отражает общую линейную зависимость между случайными величинами $X_i$ и $X_j$. Забегая вперёд скажем, что данная характеристика является абстрактной и вычисляется для **генеральных совокупностей** — всех возможных реализаций $X_i$ и $X_j$. В природе такой величины не существует, она есть только в теории вероятностей.\n",
    "\n",
    "**Выборочная корреляция** — это корреляция, вычисленная на ограниченной выборке. Это уже ближе к нашей теме. Выборочная корреляция отражает линейную взаимосвязь между факторами $\\vec{x}_{i}$ и $\\vec{x}_{j}$, реализации которых представлены в выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Корреляция фактора с самим собой всегда равна 1: $c_{ii}=1$, то есть $c_{11}=c_{22}=1$. Так происходит потому, что скалярное произведение вектора с самим собой всегда даёт 1 по свойствам скалярного произведения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> В *NumPy* матрица корреляций вычисляется функцией `np.corrcoef()`:\n",
    "\n",
    "```python\n",
    "x_1 = np.array([1, 2, 6])\n",
    "x_2 = np.array([3000, 1000, 2000])\n",
    "np.corrcoef(x_1, x_2)\n",
    "```\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e2daaabe9c82df18ac6fb0820b2d1e5a/asset-v1:SkillFactory+MIPTDS+SEPT22+type@asset+block/MATHML_md2_4_27.png)\n",
    "\n",
    "> В *Pandas* матрица корреляций вычисляется методом `corr()`, вызванным от имени *DataFrame*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике корреляция с точки зрения линейной алгебры означает следующее:\n",
    "\n",
    "* Если корреляция $c_{ij} =1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и сонаправлены.\n",
    "\n",
    "* Если корреляция $c_{ij} =-1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и противонаправлены.\n",
    "\n",
    "* Если корреляция $c_{ij} =0$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ ортогональны друг другу и, таким образом, являются линейно независимыми.\n",
    "\n",
    "Во всех остальных случаях между факторами  и  существует какая-то линейная взаимосвязь, причём чем ближе модуль коэффициента корреляции к 1, тем сильнее эта взаимосвязь.\n",
    "\n",
    "|Сила связи|Значение коэффициента корреляции|\n",
    "|----------|--------------------------------|\n",
    "|Отсутствие связи или очень слабая связь|0…+/- 0.3|\n",
    "|Слабая связь|+/- 0.3…+/- 0.5|\n",
    "|Средняя связь|+/- 0.5…+/- 0.7|\n",
    "|Сильная связь|+/- 0.7…+/- 0.9|\n",
    "|Очень сильная или абсолютная связь|+/- 0.9…+/-1|\n",
    "\n",
    "Таким образом, матрица корреляций — это матрица Грама, составленная для стандартизированных столбцов исходной матрицы наблюдений $A$. Она всегда (в теории) симметричная. На главной диагонали этой матрицы стоят 1, а на местах всех остальных элементов — коэффициенты корреляции между факторами $\\vec{x}_i$ и $\\vec{x}_j$.\n",
    "\n",
    "Если коэффициент корреляции больше 0, то взаимосвязь между факторами прямая (растёт один — растёт второй), в противном случае — обратная (растёт один — падает второй)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем**\n",
    "\n",
    "* Корреляция — это мера линейной зависимости между признаками.\n",
    "\n",
    "* Чем больше по модулю корреляция между каким-нибудь фактором и целевым признаком, тем лучше:\n",
    "\n",
    "$$\\left|corr(\\vec{x}_{i}, \\vec{y}) \\right| \\rightarrow 1 - хорошо$$\n",
    "\n",
    "* Чем больше по модулю корреляция между факторами, тем хуже:\n",
    "\n",
    "$$\\left|corr(\\vec{x}_{i}, \\vec{x}_{j}) \\right| \\rightarrow 1 - плохо$$\n",
    "\n",
    "* Чем больше линейно зависимых факторов, тем меньше ранг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выделить **два неприятных случая**:\n",
    "\n",
    "1) **Чистая коллинеарность**\n",
    "\n",
    "    Некоторые факторы являются линейно зависимыми между собой. Это влечёт к уменьшению ранга матрицы факторов. Корреляции между зависимыми факторами близки к +1 или -1. Матрица корреляции вырождена.\n",
    "\n",
    "2) **Мультиколлинеарность**\n",
    "\n",
    "    Формально линейной зависимости между факторами нет, и матрица факторов имеет максимальный ранг. Однако корреляции между мультиколлинеарными факторами по-прежнему близки к +1 или -1, и матрица корреляции практически вырождена, несмотря на то что имеет максимальный ранг.\n",
    "\n",
    "Таким образом, чистая коллинеарность провоцирует больше проблем, но её легче заметить. Мультиколлинеарность же может быть скрытой, и заметить её не так просто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "# Задание 4.7\n",
    "\n",
    "v = np.array([5, 1, 2])\n",
    "u = np.array([4, 2, 8])\n",
    "print('{:.2f}'.format(np.corrcoef(u, v)[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 3\n",
      "Determinant: 0.0000005\n",
      "[[1.         0.99925473 0.99983661]\n",
      " [0.99925473 1.         0.99906626]\n",
      " [0.99983661 0.99906626 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Задание 4.8\n",
    "\n",
    "x_1 = np.array([5.1, 1.8, 2.1, 10.3, 12.1, 12.6])\n",
    "x_2 = np.array([10.2, 3.7, 4.1, 20.5, 24.2, 24.1])\n",
    "x_3 = np.array([2.5, 0.9, 1.1, 5.1, 6.1, 6.3])\n",
    "C = np.corrcoef([x_1, x_2, x_3])\n",
    "print('Rank:', np.linalg.matrix_rank(C))\n",
    "print('Determinant: {:.7f}'.format(np.linalg.det(C)))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Полиномиальная регрессия</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
