{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Логистическая регрессия</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача классификации (classification)** — задача, в которой мы пытаемся предсказать класс объекта на основе признаков в наборе данных. То есть задача сводится к предсказанию целевого признака, который является категориальным.\n",
    "\n",
    "* Когда классов, которые мы хотим предсказать, только два, классификация называется **бинарной**.\n",
    "\n",
    "* Когда классов, которые мы хотим предсказать, более двух, классификация называется **мультиклассовой (многоклассовой)**. \n",
    "\n",
    "Что вообще означает «решить задачу классификации»? Это значит построить разделяющую поверхность в пространстве признаков, которая делит пространство на части, каждая из которых соответствует определённому классу. \n",
    "\n",
    "Модели, которые решают задачу классификации, называются **классификаторами (classifier)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель должна выдавать некоторую вероятность $P$, которая будет определять, принадлежит ли данный объект к классу 1: например, вероятность того, что письмо является спамом. При этом вероятность того, что письмо является обычным письмом (класс 0), определяется как $Q=1-P$.  \n",
    "\n",
    "Когда модель будет обучена на предсказание вероятности, мы зададим некоторый порог вероятности. Если предсказанная вероятность будет выше этого порога, мы определим объект к классу 1, а если ниже — к классу 0.\n",
    "\n",
    "Например, стандартный порог равен 0.5. То есть если вероятность $P>0.5$, мы будем считать письмо спамом, а если $P \\leq 0.5$ — обычным информативным письмом.\n",
    "\n",
    "В итоге мы добьёмся того, что будем предсказывать не дискретный категориальный, а непрерывный числовой признак, который лежит в диапазоне [0, 1]. А это уже знакомая нам задача регрессии.\n",
    "\n",
    "Остался главный вопрос: как научить модель предсказывать вероятности, ведь они должны лежать строго в диапазоне от 0 до 1, а предсказания линейной регрессии лежат в диапазоне от $-\\infty$ до $+\\infty$? \n",
    "\n",
    "Тут-то мы и приходим к модели логистической регрессии — **регрессии вероятностей**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия (Logistic Regression)** — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в Data Science.\n",
    "\n",
    "В основе логистической регрессии лежит **логистическая функция (logistic function) $\\sigma(z)$** — отсюда и название модели. Однако более распространённое название этой функции — **сигмόида (sigmoid)**. Записывается она следующим образом:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Здесь $e$ — экспонента или число Эйлера. Это число является бесконечным, а его значение обычно принимают равным 2.718...\n",
    "\n",
    "График зависимости сигмоиды от аргумента $z$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У сигмоиды есть два очень важных для нас свойства:\n",
    "\n",
    "* Значения сигмоиды $\\sigma(z)$ лежат в диапазоне от 0 до 1 при любых значения аргумента $z$: какой бы $z$ вы ни подставили, число меньше 0 или больше 1 вы не получите.\n",
    "\n",
    "* Сигмоида выдаёт значения $\\sigma(z)>0.5$ при её аргументе $z>0$, $\\sigma(z)<0.5$ — при $z<0$ и $\\sigma(z)=0.5$ — при $z=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В математике принято писать оценочные величины с «шапкой» наверху, а истинные значения — без «шапки», это чистая формальность.\n",
    "\n",
    "Мы будем называть оценки вероятности ($\\hat{P}$) просто вероятностью, но только для краткости. Это не значит, что эти оценки являются истинными вероятностями принадлежности к каждому из классов (их нельзя сосчитать, так как для этого нужна выборка бесконечного объёма). Если вы употребляете термин «вероятности» на собеседованиях, обязательно предварительно укажите, что вы подразумеваете оценку вероятности.\n",
    "\n",
    "Основная идея модели логистической регрессии: возьмём модель линейной регрессии (обозначим её выход за $z$):\n",
    "\n",
    "$$z=w_{0}+\\sum_{j=1}^{m} w_{j} x_{j}$$\n",
    "\n",
    "И подставим выход модели $z$ в функцию сигмоиды, чтобы получить искомые оценки вероятности:\\\n",
    "\n",
    "$$\\hat{P}=\\sigma(z)=\\frac{1}{1+e^{-z}}=\\frac{1}{1+e^{-w_{0}-\\sum_{j=1}^{m} w_{j} x_{j}}}=\\frac{1}{1+e^{-\\bar{w} \\cdot \\bar{x}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать будем всё в совокупности, пытаясь получить наилучшую оценку вероятности $\\hat{P}$. Если вероятность $\\hat{P} > 0.5$, относим объект к классу 1, а если $\\hat{P} \\leq 0.5$, относим объект к классу 0. \n",
    "\n",
    "Математически это записывается следующей формулой:\n",
    "\n",
    "$$\\hat{y}=I[\\hat{P}]=\\left\\{\\begin{array}{l} 1, \\hat{P}>0.5 \\\\ 0, \\hat{P} \\leq 0.5 \\end{array}\\right.$$\n",
    "\n",
    "В данном выражении $I[ \\hat{P}]$ называется **индикаторной функцией**. Она возвращает 1, если её значение больше какого-то порога, и 0 — в противном случае. Математики часто записывают просто квадратные скобки, опуская символ $I$: $[\\hat{P}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы обучим модель, то есть подберём  коэффициенты таким образом, что для объектов класса 1 модель линейной регрессии начнёт выдавать положительное число, а для класса 0 — выдавать отрицательное число, то тогда, подставив предсказание линейной регрессии $z$ в сигмоиду, мы сможем получать вероятности принадлежности к каждому из классов в диапазоне от 0 до 1.\n",
    "\n",
    "Далее по порогу вероятности мы сможем определять, к какому классу принадлежит объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевым моментом в предсказании логистической регрессии является расстояние от точки до разделяющей плоскости в пространстве факторов. Это расстояние в литературе часто называется **отступом (margin)**. \n",
    "\n",
    "Чем больше расстояние от точки, находящейся выше разделяющей плоскости, до самой плоскости, тем больше оценка вероятности принадлежности к классу 1.\n",
    "\n",
    "В общем случае, когда у нас есть зависимость от  факторов, линейное выражение, находящееся под сигмоидой, будет обозначать разделяющую гиперплоскость.\n",
    "\n",
    "$$z = w_{0} + w_{1}x_{1} + w_{2}x_{2} + ...+ w_{m}x_{m} = w_{0} + \\sum_{j=1}^{m} w_{j}x_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Поиск параметров логистической регрессии</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это **метод максимального правдоподобия (Maximum Likelihood Estimation — MLE)**. \n",
    "\n",
    "**Правдоподобие** — это оценка того, насколько вероятно получить истинное значение целевой переменной $y$ при данных $x$ и параметрах $w$. \n",
    "\n",
    "Данный метод позволяет получить функцию правдоподобия. Цель метода — найти такие параметры $w=(w_{0}, w_{1}, w_{2}, ..., w_{m})$, в которых наблюдается максимум функции правдоподобия.\n",
    "\n",
    "Конечная формула:\n",
    "\n",
    "$$likelihood = \\sum_{i}^{n} (y_{i} log (\\hat{P_{i}}) + (1-y_{i}) log (1-\\hat{P_{i}})) \\rightarrow max_{w}$$\n",
    "\n",
    "* $n$ — количество наблюдений.\n",
    "\n",
    "* $y_{i}$ — это истинный класс (1 или 0) для $i$-ого объекта из набора данных.\n",
    "\n",
    "* $\\hat{P_{i}} = \\sigma(z_{i})$ — предсказанная с помощью логистической регрессии вероятность принадлежности к классу 1 для $i$-ого объекта из набора данных.\n",
    "\n",
    "* $z_{i}$ — результат подстановки $i$-ого объекта из набора данных в уравнение разделяющей плоскости $z_{i}= \\bar{w} \\cdot \\bar{x_{i}}$.\n",
    "\n",
    "* $log$ — логарифм (обычно используется натуральный логарифм по основанию $e - ln$).\n",
    "\n",
    "К сожалению, функция *likelihood* не имеет интерпретации, то есть нельзя сказать, что значит число -2.34 в контексте правдоподобия.\n",
    "\n",
    "Цель — найти такие параметры, при которых наблюдается максимум этой функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь $L(w)$, которая носит название *«функция логистических потерь»*, или *logloss*. Также часто можно встретить название *кросс-энтропия*, или *cross-entropy loss*:\n",
    "\n",
    "$$L(w) = \\text { logloss } =-\\sum_{i}^{n} (y_{i} log (\\hat{P_{i}}) + (1-y_{i}) log (1-\\hat{P_{i}})) \\rightarrow min_{w}$$\n",
    "\n",
    "$$\\hat{P_{i}}=\\frac{1}{1+e^{-w_{0}-\\sum_{j=1}^{m} w_{j} x_{j}}}$$\n",
    "\n",
    "Мы должны найти такие параметры разделяющей плоскости  $w$, при которых наблюдается минимум *logloss*.\n",
    "\n",
    "К сожалению, для такой функции потерь аналитическое решение оптимизационной задачи найти не получится: при расчётах получается, что его попросту не существует.\n",
    "\n",
    "Но мы помним, что, помимо аналитических решений, есть и численные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется регуляризация. В реализации логистической регрессии в `sklearn` она немного отличается от той, что мы видели ранее для линейной регрессии.\n",
    "\n",
    "При *L1*-регуляризации мы добавляем в функцию потерь $L(w)$ штраф из суммы модулей параметров, а саму функцию *logloss* умножаем на коэффициент $C$:\n",
    "\n",
    "$$L(w)=C \\cdot \\log \\operatorname{loss}+\\sum_{j=1}^{m}\\left|w_{j}\\right| \\rightarrow \\min_{w}$$\n",
    "\n",
    "А при *L2*-регуляризации — штраф из суммы квадратов параметров:\n",
    "\n",
    "$$L(w)=C \\cdot \\log \\operatorname{loss}+\\sum_{j=1}^{m}\\left(w_{j}\\right)^{2} \\rightarrow \\min _{w}$$\n",
    "\n",
    "Значение коэффициента $C$ — коэффициент, обратный коэффициенту регуляризации. Чем больше $C$, тем меньше «сила» регуляризации. Коэффициент $C$ — это коэффициент, обратный коэффициенту регуляризации $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Логистическая регрессия в Sklearn</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Смотри блокнот \"Логистическая_регрессия\"*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
