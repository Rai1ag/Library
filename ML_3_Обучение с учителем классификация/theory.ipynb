{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Логистическая регрессия</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача классификации (classification)** — задача, в которой мы пытаемся предсказать класс объекта на основе признаков в наборе данных. То есть задача сводится к предсказанию целевого признака, который является категориальным.\n",
    "\n",
    "* Когда классов, которые мы хотим предсказать, только два, классификация называется **бинарной**.\n",
    "\n",
    "* Когда классов, которые мы хотим предсказать, более двух, классификация называется **мультиклассовой (многоклассовой)**. \n",
    "\n",
    "Что вообще означает «решить задачу классификации»? Это значит построить разделяющую поверхность в пространстве признаков, которая делит пространство на части, каждая из которых соответствует определённому классу. \n",
    "\n",
    "Модели, которые решают задачу классификации, называются **классификаторами (classifier)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель должна выдавать некоторую вероятность $P$, которая будет определять, принадлежит ли данный объект к классу 1: например, вероятность того, что письмо является спамом. При этом вероятность того, что письмо является обычным письмом (класс 0), определяется как $Q=1-P$.  \n",
    "\n",
    "Когда модель будет обучена на предсказание вероятности, мы зададим некоторый порог вероятности. Если предсказанная вероятность будет выше этого порога, мы определим объект к классу 1, а если ниже — к классу 0.\n",
    "\n",
    "Например, стандартный порог равен 0.5. То есть если вероятность $P>0.5$, мы будем считать письмо спамом, а если $P \\leq 0.5$ — обычным информативным письмом.\n",
    "\n",
    "В итоге мы добьёмся того, что будем предсказывать не дискретный категориальный, а непрерывный числовой признак, который лежит в диапазоне [0, 1]. А это уже знакомая нам задача регрессии.\n",
    "\n",
    "Остался главный вопрос: как научить модель предсказывать вероятности, ведь они должны лежать строго в диапазоне от 0 до 1, а предсказания линейной регрессии лежат в диапазоне от $-\\infty$ до $+\\infty$? \n",
    "\n",
    "Тут-то мы и приходим к модели логистической регрессии — **регрессии вероятностей**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия (Logistic Regression)** — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в Data Science.\n",
    "\n",
    "В основе логистической регрессии лежит **логистическая функция (logistic function) $\\sigma(z)$** — отсюда и название модели. Однако более распространённое название этой функции — **сигмόида (sigmoid)**. Записывается она следующим образом:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Здесь $e$ — экспонента или число Эйлера. Это число является бесконечным, а его значение обычно принимают равным 2.718...\n",
    "\n",
    "График зависимости сигмоиды от аргумента $z$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У сигмоиды есть два очень важных для нас свойства:\n",
    "\n",
    "* Значения сигмоиды $\\sigma(z)$ лежат в диапазоне от 0 до 1 при любых значения аргумента $z$: какой бы $z$ вы ни подставили, число меньше 0 или больше 1 вы не получите.\n",
    "\n",
    "* Сигмоида выдаёт значения $\\sigma(z)>0.5$ при её аргументе $z>0$, $\\sigma(z)<0.5$ — при $z<0$ и $\\sigma(z)=0.5$ — при $z=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В математике принято писать оценочные величины с «шапкой» наверху, а истинные значения — без «шапки», это чистая формальность.\n",
    "\n",
    "Мы будем называть оценки вероятности ($\\hat{P}$) просто вероятностью, но только для краткости. Это не значит, что эти оценки являются истинными вероятностями принадлежности к каждому из классов (их нельзя сосчитать, так как для этого нужна выборка бесконечного объёма). Если вы употребляете термин «вероятности» на собеседованиях, обязательно предварительно укажите, что вы подразумеваете оценку вероятности.\n",
    "\n",
    "Основная идея модели логистической регрессии: возьмём модель линейной регрессии (обозначим её выход за $z$):\n",
    "\n",
    "$$z=w_{0}+\\sum_{j=1}^{m} w_{j} x_{j}$$\n",
    "\n",
    "И подставим выход модели $z$ в функцию сигмоиды, чтобы получить искомые оценки вероятности:\\\n",
    "\n",
    "$$\\hat{P}=\\sigma(z)=\\frac{1}{1+e^{-z}}=\\frac{1}{1+e^{-w_{0}-\\sum_{j=1}^{m} w_{j} x_{j}}}=\\frac{1}{1+e^{-\\bar{w} \\cdot \\bar{x}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать будем всё в совокупности, пытаясь получить наилучшую оценку вероятности $\\hat{P}$. Если вероятность $\\hat{P} > 0.5$, относим объект к классу 1, а если $\\hat{P} \\leq 0.5$, относим объект к классу 0. \n",
    "\n",
    "Математически это записывается следующей формулой:\n",
    "\n",
    "$$\\hat{y}=I[\\hat{P}]=\\left\\{\\begin{array}{l} 1, \\hat{P}>0.5 \\\\ 0, \\hat{P} \\leq 0.5 \\end{array}\\right.$$\n",
    "\n",
    "В данном выражении $I[ \\hat{P}]$ называется **индикаторной функцией**. Она возвращает 1, если её значение больше какого-то порога, и 0 — в противном случае. Математики часто записывают просто квадратные скобки, опуская символ $I$: $[\\hat{P}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы обучим модель, то есть подберём  коэффициенты таким образом, что для объектов класса 1 модель линейной регрессии начнёт выдавать положительное число, а для класса 0 — выдавать отрицательное число, то тогда, подставив предсказание линейной регрессии $z$ в сигмоиду, мы сможем получать вероятности принадлежности к каждому из классов в диапазоне от 0 до 1.\n",
    "\n",
    "Далее по порогу вероятности мы сможем определять, к какому классу принадлежит объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевым моментом в предсказании логистической регрессии является расстояние от точки до разделяющей плоскости в пространстве факторов. Это расстояние в литературе часто называется **отступом (margin)**. \n",
    "\n",
    "Чем больше расстояние от точки, находящейся выше разделяющей плоскости, до самой плоскости, тем больше оценка вероятности принадлежности к классу 1.\n",
    "\n",
    "В общем случае, когда у нас есть зависимость от  факторов, линейное выражение, находящееся под сигмоидой, будет обозначать разделяющую гиперплоскость.\n",
    "\n",
    "$$z = w_{0} + w_{1}x_{1} + w_{2}x_{2} + ...+ w_{m}x_{m} = w_{0} + \\sum_{j=1}^{m} w_{j}x_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Поиск параметров логистической регрессии</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это **метод максимального правдоподобия (Maximum Likelihood Estimation — MLE)**. \n",
    "\n",
    "**Правдоподобие** — это оценка того, насколько вероятно получить истинное значение целевой переменной $y$ при данных $x$ и параметрах $w$. \n",
    "\n",
    "Данный метод позволяет получить функцию правдоподобия. Цель метода — найти такие параметры $w=(w_{0}, w_{1}, w_{2}, ..., w_{m})$, в которых наблюдается максимум функции правдоподобия.\n",
    "\n",
    "Конечная формула:\n",
    "\n",
    "$$likelihood = \\sum_{i}^{n} (y_{i} log (\\hat{P_{i}}) + (1-y_{i}) log (1-\\hat{P_{i}})) \\rightarrow max_{w}$$\n",
    "\n",
    "* $n$ — количество наблюдений.\n",
    "\n",
    "* $y_{i}$ — это истинный класс (1 или 0) для $i$-ого объекта из набора данных.\n",
    "\n",
    "* $\\hat{P_{i}} = \\sigma(z_{i})$ — предсказанная с помощью логистической регрессии вероятность принадлежности к классу 1 для $i$-ого объекта из набора данных.\n",
    "\n",
    "* $z_{i}$ — результат подстановки $i$-ого объекта из набора данных в уравнение разделяющей плоскости $z_{i}= \\bar{w} \\cdot \\bar{x_{i}}$.\n",
    "\n",
    "* $log$ — логарифм (обычно используется натуральный логарифм по основанию $e - ln$).\n",
    "\n",
    "К сожалению, функция *likelihood* не имеет интерпретации, то есть нельзя сказать, что значит число -2.34 в контексте правдоподобия.\n",
    "\n",
    "Цель — найти такие параметры, при которых наблюдается максимум этой функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь $L(w)$, которая носит название *«функция логистических потерь»*, или *logloss*. Также часто можно встретить название *кросс-энтропия*, или *cross-entropy loss*:\n",
    "\n",
    "$$L(w) = \\text { logloss } =-\\sum_{i}^{n} (y_{i} log (\\hat{P_{i}}) + (1-y_{i}) log (1-\\hat{P_{i}})) \\rightarrow min_{w}$$\n",
    "\n",
    "$$\\hat{P_{i}}=\\frac{1}{1+e^{-w_{0}-\\sum_{j=1}^{m} w_{j} x_{j}}}$$\n",
    "\n",
    "Мы должны найти такие параметры разделяющей плоскости  $w$, при которых наблюдается минимум *logloss*.\n",
    "\n",
    "К сожалению, для такой функции потерь аналитическое решение оптимизационной задачи найти не получится: при расчётах получается, что его попросту не существует.\n",
    "\n",
    "Но мы помним, что, помимо аналитических решений, есть и численные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется регуляризация. В реализации логистической регрессии в `sklearn` она немного отличается от той, что мы видели ранее для линейной регрессии.\n",
    "\n",
    "При *L1*-регуляризации мы добавляем в функцию потерь $L(w)$ штраф из суммы модулей параметров, а саму функцию *logloss* умножаем на коэффициент $C$:\n",
    "\n",
    "$$L(w)=C \\cdot \\log \\operatorname{loss}+\\sum_{j=1}^{m}\\left|w_{j}\\right| \\rightarrow \\min_{w}$$\n",
    "\n",
    "А при *L2*-регуляризации — штраф из суммы квадратов параметров:\n",
    "\n",
    "$$L(w)=C \\cdot \\log \\operatorname{loss}+\\sum_{j=1}^{m}\\left(w_{j}\\right)^{2} \\rightarrow \\min _{w}$$\n",
    "\n",
    "Значение коэффициента $C$ — коэффициент, обратный коэффициенту регуляризации. Чем больше $C$, тем меньше «сила» регуляризации. Коэффициент $C$ — это коэффициент, обратный коэффициенту регуляризации $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Логистическая регрессия в Sklearn</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S. Смотри блокнот \"extra\", часть 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Метрики классификации</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Ошибки I и II рода</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для унификации терминологии в машинном обучении в большинстве задач объекты класса 1 считаются объектами с наличием некоторого эффекта (болезнь есть / задолженность погашена / клиент ушёл / устройство отказало и т. д.), а объекты класса 0 — объектами с отсутствием этого эффекта (болезни нет / задолженность не погашена / клиент не ушёл / устройство работает без отказов и т. д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть некоторый пациент $x_i$, и мы хотим понять, болен ли он диабетом. С точки зрения задачи классификации мы хотим предсказать истинный класс ($y_i$) пациента.\n",
    "\n",
    "Нулевая гипотеза будет состоять в отсутствии эффекта (пациент не болен диабетом), то есть $y_i=0$, а альтернативная — в его наличии (пациент болен диабетом) , то есть $y_i=1$. В терминах статистических гипотез это будет записано так:\n",
    "\n",
    "* $H_0$: Пациент $x_i$ не болеет диабетом $y_i=0$.\n",
    "* $H_1$: Пациент $x_i$ болеет диабетом $y_i=1$.\n",
    "\n",
    "*<u>Ошибка I (первого) рода ($\\alpha$-ошибка)</u>*: отклонение нулевой гипотезы, когда она на самом деле верна, или **ложноположительный результат**. То есть мы предсказали, что пациент болен диабетом, хотя это не так.\n",
    "\n",
    "*<u>Ошибка II (второго) рода ($\\beta$-ошибка)</u>*: принятие нулевой гипотезы, когда она на самом деле ложна, или **ложноотрицательный результат**. То есть мы предсказали, что пациент здоров, хотя на самом деле он болен диабетом.\n",
    "\n",
    "Как вы можете понять, в диагностических задачах для нас критичнее ошибка II рода. Последствия будут более серьёзными, если мы примем больного пациента за здорового, чем если мы примем здорового за больного. Нам важно охватить всех потенциально больных пациентов, чтобы сделать дополнительный анализ и удостовериться в результате."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Метрики классификации</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайным образом выбрали десять пациентов из нашей таблицы и моделью `log_reg_full` предсказали для них ответы:\n",
    "\n",
    "$$y=(1, 0, 1, 1, 0, 1, 1, 0, 1, 1)$$\n",
    "\n",
    "$$\\hat{y}=(1, 1, 0, 1, 0, 0, 1, 1, 0, 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Матрица ошибок (confusion matrix)** показывает все возможные исходы совпадения и несовпадения предсказания модели с действительностью. Используется для расчёта других метрик.\n",
    "\n",
    "    * **Истинно положительные (True Positive, TP)** — это объекты, обозначенные моделью как класс 1 ($\\hat{y}=1$) и действительно принадлежащие к классу 1 ($y=1$).\n",
    "\n",
    "    * **Ложноположительные (False Positive, FP)** — это объекты, обозначенные моделью как класс 1 ($\\hat{y}=1$), но в действительности принадлежащие к классу 0 ($y=0$). То есть это объекты, для которых модель совершила ошибку I рода.\n",
    "\n",
    "    * **Истинно отрицательные (True Negative, TN)** — это объекты, обозначенные моделью как класс 0 ($\\hat{y}=0$) и действительно принадлежащие к классу 0 ($y=0$).\n",
    "\n",
    "    * **Ложноотрицательные (False Negative, FN)** — это объекты, обозначенные моделью как класс 0 ($\\hat{y}=0$), но в действительности принадлежащие к классу 1 ($y=1$). То есть это объекты, для которых модель совершила ошибку II рода.\n",
    "\n",
    "Общий вид матрицы ошибок:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формально матрица ошибок не является метрикой, но на её основе составляются сами метрики классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Accuracy (достоверность/аккуратность)** — доля правильных ответов модели среди всех ответов. Правильные ответы — это истинно положительные (*True Positive*) и истинно отрицательные ответы (*True Negative*):\n",
    "\n",
    "$$accuracy = \\frac{TP + TN}{TP + TN + FN + FP}$$\n",
    "\n",
    "*Интерпретация*: как много (в долях) модель угадала ответов.\n",
    "\n",
    "Метрика изменяется в диапазоне от 0 до 1. Чем ближе значение к 1, тем больше ответов модель «угадала». *Accuracy* — самая простая и самая понятная метрика классификации, но у неё есть один существенный недостаток. Она бесполезна, если классы сильно несбалансированы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Precision (точность)**, или **PPV (Positive Predictive Value)** — это доля объектов, названных классификатором положительными и при этом действительно являющихся таковыми, по отношению ко всем названным положительными объектам.\n",
    "\n",
    "$$precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Метрика также изменяется от 0 до 1. \n",
    "\n",
    "*Интерпретация*: способность отделить класс 1 от класса 0. Чем больше *precision*, тем меньше ложных попаданий. То есть **чем ближе *precision* к 1, тем меньше вероятность модели допустить ошибку I рода**.\n",
    "\n",
    "*Precision* нужен в задачах, где от нас требуется минимум ложных срабатываний. Чем выше «цена» ложноположительного результата, тем выше должен быть *precision*. \n",
    "\n",
    "В предельном случае (когда *precision* равен 1) у модели отсутствуют ложноположительные срабатывания. Важно понимать, что данный вывод справедлив только для выборки, на которой мы оцениваем метрику, то есть это не означает, что модель вовсе не может допустить ложноположительных результатов. Однако чем больше выборка, на которой мы тестируем алгоритм, тем ближе к истине будет данный вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **Recall (полнота)**, или **TPR (True Positive Rate)** — это доля объектов, названных классификатором положительными и при этом действительно являющихся таковыми, по отношению ко всем объектам положительного класса.\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Метрика изменяется от 0 до 1.\n",
    "\n",
    "*Интерпретация*: способность модели обнаруживать класс 1 вообще, то есть охват класса 1. Заметьте, что метрика зависит от количества ложноотрицательных срабатываний. То есть чем ближе *recall* к 1, тем меньше вероятность модели допустить ошибку II рода.\n",
    "\n",
    "*Recall* очень хорошо себя показывает в задачах, где важно найти как можно больше объектов, принадлежащих к классу 1. \n",
    "\n",
    "Предельный случай (когда *recall* равен 1) означает, что модель нашла все объекты класса 1, например всех действительно больных пациентов. Однако метрика ничего не скажет о том, с какой точностью мы это сделали. Важно понимать, что данный вывод справедлив только для выборки, на которой мы оцениваем метрику, то есть это не означает, что модель вовсе не может допустить ложноотрицательных исходов. Однако чем больше выборка, на которой мы тестируем алгоритм, тем данный вывод будет ближе к истине."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Метрики *precision* и *recall* не зависят от сбалансированности классов и в совокупности дают довольно исчерпывающее представление о классификаторе. Однако на практике часто бывает так, что увеличение одной из метрик может привести к уменьшению другой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **$F_{\\beta}$ (F-мера)** — это **взвешенное среднее гармоническое** между *precision* и *recall*:\n",
    "\n",
    "$$F_{\\beta} = (1+\\beta^{2})\\frac{precision \\cdot recall}{(\\beta^{2} precision) + recall}$$\n",
    "\n",
    "$\\beta$ - это вес *precision* в метрике: чем больше $\\beta$, тем больше вклад.\n",
    "\n",
    "В частном случае, когда $\\beta=1$, мы получаем равный вклад для *precision* и *recall*, а формула будет выражать простое среднее гармоническое, или метрику $F_{1}$ ($F_{1}$-мера):\n",
    "\n",
    "$$F_{1} = 2\\frac{precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "Метрика равна своему максимуму (1), если и *precision*, и *recall* равны 1 (то есть когда отсутствуют как ложноположительные, так и ложноотрицательные срабатывания). Но если хотя бы одна из метрик будет близка к 0, то и $F_{1}$ будет близка к 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ещё одно небольшое, но очень важное замечание: все суждения, которые мы привели по отношению к *precision*, *recall* и *$F$-мере*, относятся только к классу 1, так как эти метрики по умолчанию считаются для класса 1. Для решения большинства задач знания о значении этих метрик для класса 1 более чем достаточно, так как обычно нас интересует именно наличие некоторого эффекта.\n",
    "\n",
    "> Однако если вам по каким-то причинам необходимо рассчитать *precision*, *recall* и *$F$-меру* для класса 0, для этого достаточно сделать перекодировку классов — поменять их обозначения местами или (при расчёте метрик с помощью библиотеки `sklearn`) изменить значение специального параметра `pos_label` на 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Название|Функция в модуле metrics|\n",
    "|--------|:-----------------------|\n",
    "|Accuracy (достоверность)|accuracy_score()|\n",
    "|Precision (точность)|precision_score()|\n",
    "|Recall (полнота)|recall_score()|\n",
    "|$F_{1}$-мера|f1_score()|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Расчёт метрик на Python</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S. Смотри блокнот \"extra\", часть 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Достоинства и недостатки логистической ррегресии</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Достоинства:</u>**\n",
    "\n",
    "* Простой, интерпретируемый, но в то же время эффективный алгоритм.\n",
    "\n",
    "* Поиск параметров линейный или квадратичный (в зависимости от метода оптимизации), то есть ресурсозатратность алгоритма очень низкая.\n",
    "\n",
    "* Не требует подбора внешних параметров (гиперпараметров), так как практически не зависит от них.\n",
    "\n",
    "**<u>Недостатки:</u>**\n",
    "\n",
    "* Алгоритм работает хорошо, только когда классы линейно разделимы, что в реальных задачах бывает очень редко. Поэтому обычно данная модель используется в качестве *baseline*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Недостаток с линейной разделимостью классов можно побороть с помощью введения полиномиальных признаков, тем самым снизив смещение модели. Тогда логистическая регрессия вместо разделяющей плоскости будет означать выгнутую разделяющую поверхность сложной структуры. Однако мы знаем, что с этим трюком стоит быть аккуратным, так как можно получить переобученную модель. Поэтому в комбинации с полиномиальными признаками стоит подобрать наилучший параметр регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-3_18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Задание 3.6\n",
    "\n",
    "from sklearn import metrics #метрики\n",
    "\n",
    "y_true = [0, 0, 1, 1, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1, 1, 0, 1]\n",
    "\n",
    "print('Precision: {:.2f}'.format(metrics.precision_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Задание 3.7\n",
    "\n",
    "y_true = [0, 0, 1, 1, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1, 1, 0, 1]\n",
    "\n",
    "print('Recall: {:.2f}'.format(metrics.recall_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Задание 3.8\n",
    "\n",
    "y_true = [0, 0, 1, 1, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1, 1, 0, 1]\n",
    "\n",
    "print('F1 score: {:.2f}'.format(metrics.f1_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Мультиклассовая классификация</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать, когда классов, на которые необходимо разделить данные, больше 2? В таком случае используется очень простой подход, который называется **«один против всех» (one-vs-over)**.\n",
    "\n",
    "Идея этого подхода очень простая. Если у нас есть $k$ различных классов ($k>2$), давайте обучим $k$ классификаторов, каждый из которых будет предсказывать вероятности принадлежности каждого объекта к определённому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@ML_3_4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-3_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате у нас получится три различных пространства вероятностей, что-то вроде трёх параллельных реальностей. Чтобы собрать всё это воедино, мы выбираем в каждой точке пространства максимум из вероятностей. Получим следующую картину:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@dst3-ml3-3_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель логистической регрессии легко обобщается на случай мультиклассовой классификации. Пусть мы построили несколько разделяющих плоскостей с различными наборами параметров $k$, где $k$ — номер классификатора. То есть имеем $K$ разделяющих плоскостей:\n",
    "\n",
    "$$z_{k}=w_{0 k}+\\sum_{j=1}^{m} w_{j k} x_{j}=w_{k} \\cdot x$$\n",
    "\n",
    "Чтобы преобразовать результат каждой из построенных моделей в вероятности в логистической регрессии, используется функция **softmax** — многомерный аналог сигмоиды:\n",
    "\n",
    "$$\\hat{P}_{k}=\\operatorname{softmax}\\left(z_{k}\\right)=\\frac{\\exp \\left(\\hat{y}_{k}\\right)}{\\sum_{k=1}^{K} \\exp \\left(\\hat{y}_{j k}\\right)}$$\n",
    "\n",
    "Данная функция выдаёт нормированные вероятности, то есть в сумме для всех классов вероятность будет равна 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Мультиклассовая классификация на Python</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S. Смотри блокнот \"extra_2\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Практика</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S. Смотри блокнот \"extra_3\", часть 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "from sklearn import linear_model #линейные модели\n",
    "from sklearn import tree #деревья решений\n",
    "from sklearn import ensemble #ансамбли\n",
    "from sklearn import metrics #метрики\n",
    "from sklearn import preprocessing #предобработка\n",
    "from sklearn.model_selection import train_test_split #сплитование выборки\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.style.use('seaborn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
