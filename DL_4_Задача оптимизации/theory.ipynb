{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Список литературы:**\n",
    "\n",
    "* Djork-Arne ́ Clevert, Thomas Unterthine, Sepp Hochreite. FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS) // arxiv.org [Электронный ресурс]. URL: https://arxiv.org/pdf/1511.07289.pdf (дата обращения: 10.03.2024).\n",
    "\n",
    "* Computacion Inteligente Derivative-Based Optimization // slideplayer.com [Электронный ресурс]. URL: https://slideplayer.com/slide/7341917/ (дата обращения: 10.03.2024).\n",
    "\n",
    "* Variance // en.wikipedia.org [Электронный ресурс]. URL: https://en.wikipedia.org/wiki/Variance (дата обращения: 10.03.2024).\n",
    "\n",
    "* Xavier Glorot, Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks // proceedings.mlr [Электронный ресурс]. URL: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf (дата обращения: 10.03.2024).\n",
    "\n",
    "* Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting // www.cs.toronto.edu [Электронный ресурс]. URL: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf (дата обращения: 10.03.2024).\n",
    "\n",
    "* Beyond SGD: Gradient Descent with Momentum and Adaptive Learning Rate // agustinus.kristia.de [Электронный ресурс]. URL: https://agustinus.kristia.de/techblog/2016/06/22/nn-optimization/ (дата обращения: 10.03.2024).\n",
    "\n",
    "* Rohan Kshirsagar. Life is gradient descent // hackernoon.com [Электронный ресурс]. URL: https://hackernoon.com/life-is-gradient-descent-880c60ac1be8 (Для перехода по ссылке рекомендуем воспользоваться сервисом VPN) (дата обращения: 10.03.2024).\n",
    "\n",
    "* Why Momentum Really Works // distill.pub [Электронный ресурс]. URL: https://distill.pub/2017/momentum/(дата обращения: 10.03.2024).\n",
    "\n",
    "* Sebastian Ruder. An overview of gradient descent optimization algorithms // www.ruder.io [Электронный ресурс]. URL: https://www.ruder.io/optimizing-gradient-descent/(дата обращения: 10.03.2024)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
