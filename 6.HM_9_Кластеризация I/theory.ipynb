{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Кластеризация** — это задача группировки объектов на подмножества (кластеры) таким образом, чтобы объекты из одного кластера были более похожи друг на друга, чем на объекты из других кластеров, по какому-либо критерию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>K-Means</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # инициализируем алгоритм k-means с количеством кластеров 3\n",
    "# kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `n_clusters` — количество кластеров;\n",
    "* `n_init` — количество итераций алгоритма k-means;\n",
    "* `random_state` — параметр для воспроизводимости результатов от запуска к запуску."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Mini-Batch K-means</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная вариация k-means используется, когда данных очень много. Из-за их объёма вычисление центров по всей выборке занимает много времени.\n",
    "\n",
    "Для решения этой проблемы k-means на каждом шаге работает с небольшой подвыборкой данных. В общем случае упрощённый алгоритм должен сходиться к тому же результату, что и на полной выборке. Однако исследования показывают, что качество кластеров может ухудшаться по сравнению с классическим k-means. Обычно разница в кластеризации методом Mini-Batch K-means и классическим k-means заключается в пограничных точках близко расположенных кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # два кластера и подвыборки объёма 6\n",
    "# kmeans = MiniBatchKMeans(n_clusters=2,random_state=42,batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>K-means++</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную вариацию k-means используют, если признаков очень много.\n",
    "\n",
    "Результат и время работы алгоритма зависят от изначального выбора центроидов. Чтобы минимизировать затраты, будем действовать следующим образом:\n",
    "\n",
    "1) Первый центроид выбираем случайным образом.\n",
    "2) Для каждой точки вычисляем квадрат расстояния до ближайшего центроида из тех, что уже поставлены.\n",
    "3) Далее из этих точек выбираем следующий центроид так, чтобы вероятность выбора точки была пропорциональна вычисленному для неё квадрату расстояния.\n",
    "4) Когда все точки выбраны, реализуем k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию при запуске k-means в sklearn используется именно алгоритм k-means++. Выбор алгоритма задаётся через параметр `init`:\n",
    "\n",
    "* `init='random'` — для классической версии k-means;\n",
    "* `init='k-means++'` — для вариации k-means++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Остался важный вопрос: как определить количество кластеров?\n",
    "\n",
    "**Инерция** - формула суммы квадратов всех расстояний от точек $x_i$ до центров кластеров $C_k$, к которым принадлежат данные точки.\n",
    "\n",
    "$$J(C)=\\sum_{k=1}^{K} \\sum_{i \\in C_{k}}\\left\\|x_{i}-\\mu_{k}\\right\\|^{2} \\rightarrow \\min _{C}$$\n",
    "\n",
    "> При реализации алгоритма k-means для получения значения инерции используется атрибут `inertia_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_inertia(cluster_num, X):\n",
    "# # инициализируем алгоритм кластеризации\n",
    "#     k_means =  KMeans(n_clusters=cluster_num, random_state=42)\n",
    "# # запускаем алгоритм k-means\n",
    "#     k_means.fit(X)\n",
    "# # находим значение инерции\n",
    "#     inertia = k_means.inertia_\n",
    "# # возвращаем значение инерции\n",
    "#     return inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разумеется, чем меньше эта величина, тем лучше. Однако здесь есть большая проблема: минимальное значение этой функции будет достигаться тогда, когда количество кластеров будет равняться количеству объектов (т. е. каждый кластер будет состоять из одной точки и расстояния будут нулевыми). Это уже будет ситуация переобучения, так как алгоритм чересчур сильно подстроится под данные.\n",
    "\n",
    "Для решения этой проблемы была выведена следующая эвристика: берётся такое число кластеров, начиная с которого значение функционала $J(C)$ уменьшается уже не так быстро. Формально это можно записать следующим образом:\n",
    "\n",
    "$$D(k)=\\frac{\\left|J\\left(C_{k}\\right)-J\\left(C_{k+1}\\right)\\right|}{\\left|J\\left(C_{k-1}\\right)-J\\left(C_{k}\\right)\\right|} \\rightarrow \\min _{k}$$\n",
    "\n",
    "Визуально это можно представить так:\n",
    "\n",
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@MATHML_md10_2_11.png)\n",
    "\n",
    "Можно увидеть, что инерция очень сильно уменьшается при увеличении числа кластеров с 1 до 2 и с 2 до 3 и уже не так значительно — при изменении $k$ с 3 до 4. То есть перегиб здесь находится в точке 3, и это значит, что три кластера — оптимальный вариант.\n",
    "\n",
    "> Если в ходе решения задачи вы встречаете график, на котором невозможно найти «локоть», на помощь придёт **коэффициент силуэта**.\n",
    "\n",
    "Для того чтобы его вычислить, используется следующая формула:\n",
    "\n",
    "$$s_i=\\frac{(b_i-a_i)}{max(a_i,b_i)}$$\n",
    "\n",
    "* $a_i$ — среднее расстояние от данного объекта $x_i$ до объектов из того же кластера;\n",
    "* $b_i$ — среднее расстояние от данного объекта $x_i$ до объектов из другого ближайшего кластера.\n",
    "\n",
    "Для вычисления коэффициента силуэта используется `silhouette_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_silhouette(cluster_num, X):\n",
    "#     k_means =  KMeans(n_clusters=cluster_num, random_state=42)\n",
    "#     k_means.fit(X)\n",
    "# # подсчитаем метрику силуэта, передав данные и то, к каким кластерам относятся объекты\n",
    "#     silhouette = silhouette_score(X, k_means.labels_)\n",
    "#     return silhouette\n",
    "\n",
    "# silhouette = []\n",
    "# for clust_num in range(2, 10):\n",
    "#     silhouette.append(get_silhouette(clust_num, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Коэффициент силуэта обладает следующими свойствами:</u>\n",
    "\n",
    "* Значение коэффициента находится в диапазоне от -1 до +1, где высокое значение указывает, что объект хорошо согласуется с кластером, которому он принадлежит, и плохо согласуется с «чужими» кластерами.\n",
    "\n",
    "* Если у подавляющего большинства объектов этот коэффициент высокий, то можно считать кластеризацию достаточно качественной.\n",
    "\n",
    "* Если же у большого числа объектов низкий или отрицательный коэффициент силуэта, то, возможно, кластеров слишком много/мало или данные просто плохо поддаются разделению на кластеры.\n",
    "\n",
    "Для получения итогового значения рассчитывается среднее значение силуэта для всего датасета. Соответственно, для определения оптимального количества кластеров мы будем искать самую высокую точку на графике коэффициента силуэта (ведь чем больше коэффициент, тем лучше).\n",
    "\n",
    "![image.png](https://lms.skillfactory.ru/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block@MATHML_md10_2_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>EM-алгоритм</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда данные распределены в форме вытянутых эллипсов, k-means не справляется с кластеризацией:\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/72322fbdd9951ad77a85c07760313285/asset-v1:Skillfactory+DSMED+2023+type@asset+block/MATHML_md10_3_1.png)\n",
    "\n",
    "В таком случае в качестве альтернативы можно взять один из алгоритмов кластеризации EM (Expectation-maximization) — **модель гауссовой смеси (Gaussian Mixture Model, GMM)**, в которой данные описываются законом нормального распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём два случайно сгенерированных распределения Гаусса (изображены на рисунке ниже синим и жёлтым цветом).\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/b41662d3d6b1a35a19cb5299d01aa168/asset-v1:Skillfactory+DSMED+2023+type@asset+block/MATHML_md10_3_3.png)\n",
    "\n",
    "Итак, каждая точка с какими-то вероятностями принадлежит к жёлтому и синему распределению. Теперь мы сможем использовать значения этих точек для расчёта новых математических ожиданий и дисперсий и построения новых распределений. То есть, по сути, дальше мы изменим распределения так, чтобы они максимально соответствовали точкам, которые мы к ним отнесли.\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e2ef0540953f34679fb32c764e23c034/asset-v1:Skillfactory+DSMED+2023+type@asset+block/MATHML_md10_3_7.png)\n",
    "\n",
    "Далее мы ещё раз повторяем описанную ранее процедуру и снова обновляем распределения. После ряда итераций окончательные распределения будут выглядеть так:\n",
    "\n",
    "![image.png](https://lms-cdn.skillfactory.ru/assets/courseware/v1/0022dadcc042dbd3d42a74877d4e2226/asset-v1:Skillfactory+DSMED+2023+type@asset+block/MATHML_md10_3_8.png)\n",
    "\n",
    "Таким образом, мы смогли кластеризовать наши точки: первые четыре относятся к жёлтому кластеру, а остальные — к синему. Так и работает EM-алгоритм кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В более общем виде последовательность действий в EM-алгоритме можно сформулировать следующим образом:\n",
    "\n",
    "1) Выбрать количество кластеров, которое кажется нам оптимальным для наших данных.\n",
    "2) Случайным образом выбрать параметры распределений в пространстве данных.\n",
    "3) Для каждой точки набора данных рассчитать вероятность принадлежности к каждому кластеру (**E-шаг (expectation)** — вычисляем ожидаемый кластер для каждого объекта.).\n",
    "4) Обновить параметры распределений таким образом, чтобы максимизировать вероятность принадлежности точек, отнесённых к кластеру (**M-шаг (maximization)** — оцениваем вес (априорную вероятность) и параметры распределения для каждого кластера.).\n",
    "5) Повторять шаги 3-4 фиксированное число раз либо до тех пор, пока центроиды не сойдутся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы отмечали ранее, алгоритм EM очень похож на k-means и, по сути, является его упрощённым вариантом. Однако у этих алгоритмов есть различия:\n",
    "\n",
    "* кластеры в EM эллиптические, а в k-means — сферические;\n",
    "\n",
    "* в EM кластеризация мягкая (определяем вероятность принадлежности объекта к клластреу), а в k-means — жёсткая (определяем конкретный кластер для объекта)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации алгоритма в *sklearn* мы используем `GaussianMixture`. Для запуска алгоритма `GaussianMixture` необходимо задать следующие основные параметры:\n",
    "\n",
    "* `n_components` — количество кластеров;\n",
    "* `random_state` — так как в алгоритме есть случайность при инициализации, то для воспроизводимости результатов от запуска к запуску следует передать какое-то число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Практика</center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
